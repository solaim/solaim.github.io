{"meta":{"title":"Coronarium","subtitle":"Drill down","description":"专注、探索、积累","author":"Bray Wang","url":"https://solaim.github.io","root":"/"},"pages":[{"title":"about","date":"2023-09-10T10:35:18.000Z","updated":"2023-09-10T13:21:16.190Z","comments":true,"path":"about/index.html","permalink":"https://solaim.github.io/about/index.html","excerpt":"","text":"我是干什么的？ 就是进击的程序猿呀！ 我关注的技术服务端开发分布式理论"},{"title":"index","date":"2023-09-10T10:32:30.000Z","updated":"2023-09-10T10:54:19.056Z","comments":true,"path":"index/index.html","permalink":"https://solaim.github.io/index/index.html","excerpt":"","text":""},{"title":"list","date":"2023-09-10T10:46:51.000Z","updated":"2023-09-10T10:46:51.235Z","comments":true,"path":"list/index.html","permalink":"https://solaim.github.io/list/index.html","excerpt":"","text":""}],"posts":[{"title":"Cmake极速入门","slug":"Cmake极速入门","date":"2023-01-17T08:19:47.000Z","updated":"2023-09-10T11:05:27.791Z","comments":true,"path":"2023/01/17/Cmake极速入门/","link":"","permalink":"https://solaim.github.io/2023/01/17/Cmake%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"说说k8s网络","slug":"说说k8s网络","date":"2021-06-06T03:17:45.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2021/06/06/说说k8s网络/","link":"","permalink":"https://solaim.github.io/2021/06/06/%E8%AF%B4%E8%AF%B4k8s%E7%BD%91%E7%BB%9C/","excerpt":"","text":"1、k8s组件 k8s的组件分为两大部分：控制平面组件和节点组件。 控制平面组件包括： kube-apiserver：API server，可以水平扩展。 etcd：后端存储。 kube-scheduler：决策新创建的Pod在哪个Node上运行。 kube-controller-manager：controller进程，观测Object的状态，current state -&gt; desired state。 节点组件包括： kubelet：运行Pod。 kube-proxy：网络代理， k8s中Service的部分实现，负责维护节点上的网络规则。 Container runtime：Docker、containerd、CRI-O。 2、k8s网络需要解决的问题k8s对任何CNI实现有如下规定： Pod与Pod之间的通信无须使用NAT 节点与Pod之间的通信无须使用NAT Pod与其他Pod看到自身的IP相同 k8s中的Service负责对内、对外暴露网络访问，想让应用能够在集群内外正常访问，需要解决下面的几个问题： 容器与容器的通信 Pod与Pod的通信 Pod与Service的通信 Internet与Service的通信 3、容器与容器的通信在Linux中，使用Namespace机制实现内核级别资源的隔离，目前提供六种Namespace： Mount: 隔离文件系统挂载点 UTS: 隔离主机名和域名信息 IPC: 隔离进程间通信 PID: 隔离进程的ID Network: 隔离网络资源 User: 隔离用户和用户组的ID 网络命名空间netns存在一个root network namespace，默认情况下，Linux将每个进程的netns设置为root，以提供网络访问。 对于Docker，Pod使用同一个网络命名空间，Pod之间通过Namespace隔离，Pod内的容器共享网络命名空间，Docker负责创建网络命名空间，应用容器使用-network加入该网络命名空间，容器之间通过localhost通信。 4、Pod与Pod的通信k8s中，每个Pod拥有一个真实的IP，Pod之间通过IP通信。 （本节图片来源：Kevin Sookocheff Blog） 5、Pod与Service的通信Pod随着时间，可能消失、重启，那么直接使用Pod IP进行访问，在动态变化的环境中会存在网络问题。k8s中使用Service解决这个问题。（本节图片来源：Kevin Sookocheff Blog） 创建一个新的Service Object，实际上是创建了一个虚拟IP和一系列网络规则。 kube-proxy负责维护、更新、删除、添加网络规则，其支持的代理模式有： userspace iptables（默认） ipvs kernelspace (windows) 6、Internet与Service通信","categories":[{"name":"k8s","slug":"k8s","permalink":"https://solaim.github.io/categories/k8s/"}],"tags":[{"name":"Golang, Kubernetes, Service","slug":"Golang-Kubernetes-Service","permalink":"https://solaim.github.io/tags/Golang-Kubernetes-Service/"}]},{"title":"Prometheus内部实现(四)","slug":"Prometheus内部实现-四","date":"2021-03-22T15:33:57.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2021/03/22/Prometheus内部实现-四/","link":"","permalink":"https://solaim.github.io/2021/03/22/Prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0-%E5%9B%9B/","excerpt":"","text":"目标该怎么管理？静态还是动态发现？ 目标管理是使用静态还是动态发现，这个和基础架构有关，要回答这个问题，需要先说说应用部署的现状！ 目前，企业应用部署有两种选项： 基于虚拟机 基于容器 当然，更多的是虚拟机里装docker，算是两者的结合，节省成本。 那么基于虚拟机部署应用有什么特点呢？就是变化小，你的预期和实际情况是相对静止的，在虚拟机上部署一套应用，一套流程走下来，大概得小半天吧，这种情况把要监控的目标写到Prometheus的配置文件，再重启Prometheus server，完全玩得转！ 然而，当公司的应用开始基于容器部署，特别是管控平面使用了k8s，那么手动配置Prometheus、重启就成为了噩梦一样的存在！运维会疲于奔命，开发会吐槽运维。 所以监控目标的配置，取决于使用的底层技术！ 题外话：要想Devops在企业落地，得有一整套配套的工具，整体打通。 那么Prometheus在抓取目标方面是怎么做的呢？我们先重温一下相关组件的启动： &#123; // Scrape discovery manager. g.Add( func() error &#123; err := discoveryManagerScrape.Run() level.Info(logger).Log(&quot;msg&quot;, &quot;Scrape discovery manager stopped&quot;) return err &#125;, func(err error) &#123; level.Info(logger).Log(&quot;msg&quot;, &quot;Stopping scrape discovery manager...&quot;) cancelScrape() &#125;, ) &#125; &#123; // Notify discovery manager. ... &#125; &#123; // Scrape manager. g.Add( func() error &#123; // When the scrape manager receives a new targets list // it needs to read a valid config for each job. // It depends on the config being in sync with the discovery manager so // we wait until the config is fully loaded. &lt;-reloadReady.C err := scrapeManager.Run(discoveryManagerScrape.SyncCh()) level.Info(logger).Log(&quot;msg&quot;, &quot;Scrape manager stopped&quot;) return err &#125;, func(err error) &#123; // Scrape manager needs to be stopped before closing the local TSDB // so that it doesn&#39;t try to write samples to a closed storage. level.Info(logger).Log(&quot;msg&quot;, &quot;Stopping scrape manager...&quot;) scrapeManager.Stop() &#125;, ) &#125; 启动的核心语句： # 启动动态发现 discoveryManagerScrape.Run() # 启动目标抓取 scrapeManager.Run(discoveryManagerScrape.SyncCh()) discoveryManagerScrape.SyncCh()作为参数传递给scrapeManager.Run，来看看它的实现： // SyncCh returns a read only channel used by all the clients to receive target updates. func (m *Manager) SyncCh() &lt;-chan map[string][]*targetgroup.Group &#123; return m.syncCh &#125; 实际上，是将discoveryManagerScrape和scrapeManager通过map[string][]*targetgroup.Groupchannel 连接了起来，也就是discoveryManagerScrape动态发现的目标通过这个channel，同步给scrapeManager，scrapeManager负责抓取。 我们来做一个小结： 其实，就是两个goroutine之间通过channel通信。 接下来，让我们看看scrapeManager.Run里面的实现： // Run receives and saves target set updates and triggers the scraping loops reloading. // Reloading happens in the background so that it doesn&#39;t block receiving targets updates. func (m *Manager) Run(tsets &lt;-chan map[string][]*targetgroup.Group) error &#123; go m.reloader() for &#123; select &#123; case ts := &lt;-tsets: m.updateTsets(ts) select &#123; case m.triggerReload &lt;- struct&#123;&#125;&#123;&#125;: default: &#125; case &lt;-m.graceShut: return nil &#125; &#125; &#125; 核心是一个for循环，channel tsets(map[string][]*targetgroup.Group)等待动态发现传递的目标，调用m.updateTsets(ts)更新抓取的目标，设置reload信号m.triggerReload &lt;- struct&#123;&#125;&#123;&#125;，周而复始，先来看看m.updateTsets(ts)坐了什么操作： func (m *Manager) updateTsets(tsets map[string][]*targetgroup.Group) &#123; m.mtxScrape.Lock() m.targetSets = tsets m.mtxScrape.Unlock() &#125; 没错，很简单，将监控的目标进行了更新。再来，看看Manager结构体的定义： type Manager struct &#123; logger log.Logger append storage.Appendable graceShut chan struct&#123;&#125; jitterSeed uint64 // Global jitterSeed seed is used to spread scrape workload across HA setup. mtxScrape sync.Mutex // Guards the fields below. scrapeConfigs map[string]*config.ScrapeConfig scrapePools map[string]*scrapePool targetSets map[string][]*targetgroup.Group triggerReload chan struct&#123;&#125; &#125; 可以看到跟抓取有关的三个字段为： scrapeConfigs map[string]*config.ScrapeConfig scrapePools map[string]*scrapePool targetSets map[string][]*targetgroup.Group 现在，discoveryManagerScrape通过channel将抓取的目标传递过来，scrapeManager内部到底是怎样转化成最后的HTTP请求呢？ scrapeManager.Run有一个重要的语句go m.reloader()，并在接收目标后设置了triggerReload func (m *Manager) reloader() &#123; ticker := time.NewTicker(5 * time.Second) defer ticker.Stop() for &#123; select &#123; case &lt;-m.graceShut: return case &lt;-ticker.C: select &#123; case &lt;-m.triggerReload: m.reload() case &lt;-m.graceShut: return &#125; &#125; &#125; &#125; Prometheus server每隔5秒，检查triggerReload channel，调用m.reload() func (m *Manager) reload() &#123; m.mtxScrape.Lock() var wg sync.WaitGroup for setName, groups := range m.targetSets &#123; if _, ok := m.scrapePools[setName]; !ok &#123; scrapeConfig, ok := m.scrapeConfigs[setName] if !ok &#123; level.Error(m.logger).Log(&quot;msg&quot;, &quot;error reloading target set&quot;, &quot;err&quot;, &quot;invalid config id:&quot;+setName) continue &#125; sp, err := newScrapePool(scrapeConfig, m.append, m.jitterSeed, log.With(m.logger, &quot;scrape_pool&quot;, setName)) if err != nil &#123; level.Error(m.logger).Log(&quot;msg&quot;, &quot;error creating new scrape pool&quot;, &quot;err&quot;, err, &quot;scrape_pool&quot;, setName) continue &#125; m.scrapePools[setName] = sp &#125; wg.Add(1) // Run the sync in parallel as these take a while and at high load can&#39;t catch up. go func(sp *scrapePool, groups []*targetgroup.Group) &#123; sp.Sync(groups) wg.Done() &#125;(m.scrapePools[setName], groups) &#125; m.mtxScrape.Unlock() wg.Wait() &#125; 这个方法完成了核心的转换，将目标转换成可以执行的任务，抽出核心业务我们可以得到这样的范式： var wg sync.WaitGroup for _, t := range tsets &#123; wg.Add(1) go func(t int) &#123; wg.Done() &#125;(t) &#125; wg.Wait() 回到业务上，我们发现targetSets、scrapeConfigs和scrapePools进行了完美的转换，如果scrapePools没有setName，并且scrapeConfigs有相应的配置，则调用newScrapePool创建scrapePool，更新到Manager，最后将scrapePool和[]*targetgroup.Group传递给匿名函数，完成sp.Sync(groups)调用，然我们继续跟踪Sync方法： func (sp *scrapePool) Sync(tgs []*targetgroup.Group) &#123; sp.mtx.Lock() defer sp.mtx.Unlock() start := time.Now() sp.targetMtx.Lock() var all []*Target sp.droppedTargets = []*Target&#123;&#125; for _, tg := range tgs &#123; targets, err := targetsFromGroup(tg, sp.config) if err != nil &#123; level.Error(sp.logger).Log(&quot;msg&quot;, &quot;creating targets failed&quot;, &quot;err&quot;, err) continue &#125; for _, t := range targets &#123; if t.Labels().Len() &gt; 0 &#123; all = append(all, t) &#125; else if t.DiscoveredLabels().Len() &gt; 0 &#123; sp.droppedTargets = append(sp.droppedTargets, t) &#125; &#125; &#125; sp.targetMtx.Unlock() sp.sync(all) targetSyncIntervalLength.WithLabelValues(sp.config.JobName).Observe( time.Since(start).Seconds(), ) targetScrapePoolSyncsCounter.WithLabelValues(sp.config.JobName).Inc() &#125; Sync方法的核心有两个调用： # 将Group转化成Target列表 targets, err := targetsFromGroup(tg, sp.config) # 同步 sp.sync(all) Target的定义如下： type Target struct &#123; // Labels before any processing. discoveredLabels labels.Labels // Any labels that are added to this target and its metrics. labels labels.Labels // Additional URL parameters that are part of the target URL. params url.Values mtx sync.RWMutex lastError error lastScrape time.Time lastScrapeDuration time.Duration health TargetHealth metadata MetricMetadataStore &#125; 这就是要抓取一个目标的所有配置，基本信息都包含在这里面了，现在配置转化成了可以执行的目标，我们看看sp.sync的具体实现： // sync takes a list of potentially duplicated targets, deduplicates them, starts // scrape loops for new targets, and stops scrape loops for disappeared targets. // It returns after all stopped scrape loops terminated. func (sp *scrapePool) sync(targets []*Target) &#123; var ( uniqueLoops = make(map[uint64]loop) interval = time.Duration(sp.config.ScrapeInterval) timeout = time.Duration(sp.config.ScrapeTimeout) limit = int(sp.config.SampleLimit) honorLabels = sp.config.HonorLabels honorTimestamps = sp.config.HonorTimestamps mrc = sp.config.MetricRelabelConfigs ) sp.targetMtx.Lock() for _, t := range targets &#123; hash := t.hash() if _, ok := sp.activeTargets[hash]; !ok &#123; s := &amp;targetScraper&#123;Target: t, client: sp.client, timeout: timeout&#125; l := sp.newLoop(scrapeLoopOptions&#123; target: t, scraper: s, limit: limit, honorLabels: honorLabels, honorTimestamps: honorTimestamps, mrc: mrc, &#125;) sp.activeTargets[hash] = t sp.loops[hash] = l uniqueLoops[hash] = l &#125; else &#123; // This might be a duplicated target. if _, ok := uniqueLoops[hash]; !ok &#123; uniqueLoops[hash] = nil &#125; // Need to keep the most updated labels information // for displaying it in the Service Discovery web page. sp.activeTargets[hash].SetDiscoveredLabels(t.DiscoveredLabels()) &#125; &#125; var wg sync.WaitGroup // Stop and remove old targets and scraper loops. for hash := range sp.activeTargets &#123; if _, ok := uniqueLoops[hash]; !ok &#123; wg.Add(1) go func(l loop) &#123; l.stop() wg.Done() &#125;(sp.loops[hash]) delete(sp.loops, hash) delete(sp.activeTargets, hash) &#125; &#125; sp.targetMtx.Unlock() targetScrapePoolTargetsAdded.WithLabelValues(sp.config.JobName).Set(float64(len(uniqueLoops))) forcedErr := sp.refreshTargetLimitErr() for _, l := range sp.loops &#123; l.setForcedError(forcedErr) &#125; for _, l := range uniqueLoops &#123; if l != nil &#123; go l.run(interval, timeout, nil) &#125; &#125; // Wait for all potentially stopped scrapers to terminate. // This covers the case of flapping targets. If the server is under high load, a new scraper // may be active and tries to insert. The old scraper that didn&#39;t terminate yet could still // be inserting a previous sample set. wg.Wait() &#125; 这个方法现将Target转换成Loop，然后调用go l.run(interval, timeout, nil)开始抓取： type loop interface &#123; run(interval, timeout time.Duration, errc chan&lt;- error) setForcedError(err error) stop() getCache() *scrapeCache disableEndOfRunStalenessMarkers() &#125; loop是一个interface，只要实现相应的方法都是loop。 run方法核心的在于调用scraper的scrape方法和report方法，scrape负责抓取，report负责数据上报，进行下一步的存储，scraper的定义如下： type scraper interface &#123; scrape(ctx context.Context, w io.Writer) (string, error) Report(start time.Time, dur time.Duration, err error) offset(interval time.Duration, jitterSeed uint64) time.Duration &#125; 具体实现的是targetScraper，定义如下： type targetScraper struct &#123; *Target client *http.Client req *http.Request timeout time.Duration gzipr *gzip.Reader buf *bufio.Reader &#125; func (s *targetScraper) scrape(ctx context.Context, w io.Writer) (string, error) &#123; if s.req == nil &#123; req, err := http.NewRequest(&quot;GET&quot;, s.URL().String(), nil) if err != nil &#123; return &quot;&quot;, err &#125; req.Header.Add(&quot;Accept&quot;, acceptHeader) req.Header.Add(&quot;Accept-Encoding&quot;, &quot;gzip&quot;) req.Header.Set(&quot;User-Agent&quot;, userAgentHeader) req.Header.Set(&quot;X-Prometheus-Scrape-Timeout-Seconds&quot;, fmt.Sprintf(&quot;%f&quot;, s.timeout.Seconds())) s.req = req &#125; resp, err := s.client.Do(s.req.WithContext(ctx)) if err != nil &#123; return &quot;&quot;, err &#125; defer func() &#123; io.Copy(ioutil.Discard, resp.Body) resp.Body.Close() &#125;() if resp.StatusCode != http.StatusOK &#123; return &quot;&quot;, errors.Errorf(&quot;server returned HTTP status %s&quot;, resp.Status) &#125; if resp.Header.Get(&quot;Content-Encoding&quot;) != &quot;gzip&quot; &#123; _, err = io.Copy(w, resp.Body) if err != nil &#123; return &quot;&quot;, err &#125; return resp.Header.Get(&quot;Content-Type&quot;), nil &#125; if s.gzipr == nil &#123; s.buf = bufio.NewReader(resp.Body) s.gzipr, err = gzip.NewReader(s.buf) if err != nil &#123; return &quot;&quot;, err &#125; &#125; else &#123; s.buf.Reset(resp.Body) if err = s.gzipr.Reset(s.buf); err != nil &#123; return &quot;&quot;, err &#125; &#125; _, err = io.Copy(w, s.gzipr) s.gzipr.Close() if err != nil &#123; return &quot;&quot;, err &#125; return resp.Header.Get(&quot;Content-Type&quot;), nil &#125; 我们终于看到，最后的最后，抓取其实是一个HTTP请求。为了完成一个抓取，Prometheus server进行了复杂的转化过程，正是这种实现，才让Prometheus拥有动态管理抓取目标的能力，我们大概来回忆一下这个转化过程： Group -------- | \\ | -&gt; scrapePool -&gt; pool -&gt; scraper | / Tatget ------- 终于，我们把Prometheus server的抓取过程翻了个底朝天，裨益甚多。 那么数据抓取过来就是存储、分析和展示了，下一篇我们开始研究存储。","categories":[{"name":"Golang","slug":"Golang","permalink":"https://solaim.github.io/categories/Golang/"}],"tags":[{"name":"Golang, Prometheus, Observability","slug":"Golang-Prometheus-Observability","permalink":"https://solaim.github.io/tags/Golang-Prometheus-Observability/"}]},{"title":"Prometheus内部实现(三)","slug":"Prometheus内部实现-三","date":"2021-03-20T09:43:10.000Z","updated":"2023-09-10T13:21:16.215Z","comments":true,"path":"2021/03/20/Prometheus内部实现-三/","link":"","permalink":"https://solaim.github.io/2021/03/20/Prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0-%E4%B8%89/","excerpt":"","text":"非线性 &#x3D;&gt; 线性，混乱 &#x3D;&gt; 秩序 上一篇博文主要写了Prometheus server的启动过程，但是并没有深入各个组件，了解其具体实现，这篇文件主要介绍Initial configuration loading部分。 Prometheus server的main函数有两个很精彩的实现，一个是reloader的注册，一个是组件启动。但是有个问题，所有的组件都是同时启动的吗？还是有什么顺序？ 我们列一个组件清单： - Termination handler. - Scrape discovery manager. - Notify discovery manager. - Scrape manager. - Reload handler. - Rule manager. - TSDB. - Web handler. - Notifier. 再来看一下组件启动使用的底层技术: run.Group is a universal mechanism to manage goroutine lifecycles. Create a zero-value run.Group, and then add actors to it. Actors are defined as a pair of functions: an execute function, which should run synchronously; and an interrupt function, which, when invoked, should cause the execute function to return. Finally, invoke Run, which concurrently runs all of the actors, waits until the first actor exits, invokes the interrupt functions, and finally returns control to the caller only once all actors have returned. This general-purpose API allows callers to model pretty much any runnable task, and achieve well-defined lifecycle semantics for the group. run.Group was written to manage component lifecycles in func main for OK Log. But it’s useful in any circumstance where you need to orchestrate multiple goroutines as a unit whole. Click here to see a video of a talk where run.Group is described. source: https://pkg.go.dev/github.com/oklog/run 简单总结一下，oklog&#x2F;run库的run.Group实现了一种goroutine生命的管理机制，使用了actor模型，注册一对函数execute和interrupt。回忆一下上篇博客，实现正是如此。 Termination handler组件的execute函数使用select等待term、webHandler、cancel的通知，用于保证系统正常退出，进入即阻塞。 Scrape discovery manager组件直接调用discoveryManagerScrape.Run。 Notify discovery manager组件直接调用discoveryManagerNotify.Run。 Scrape manager组件等待reloadReady.C通知，然后运行scrapeManager.Run。 Reload handler组件等待reloadReady.C通知，然后等待hup、webHandler、cancel的通知，根据通知重新加载。 Initial configuration loading组件进入后等待dbOpen、cancel通知，dbOpen通知后，加载配置。 Rule manager组件等待reloadReady.C通知，然后运行ruleManager.Run。 TSDB组件运行openDBWithMetrics，然后close dbOpen channel。 Web handler组件直接调用webHandler.Run。 Notifier组件等待reloadReady.C通知，然后调用notifierManager.Run。 可以看出整个控制流程有两个很重要的channel：dbOpen和reloadReady.C。我们再看看这两个channel， // Start all components while we wait for TSDB to open but only load // initial config and mark ourselves as ready after it completed. dbOpen := make(chan struct&#123;&#125;) // sync.Once is used to make sure we can close the channel at different execution stages(SIGTERM or when the config is loaded). type closeOnce struct &#123; C chan struct&#123;&#125; once sync.Once Close func() &#125; // Wait until the server is ready to handle reloading. reloadReady := &amp;closeOnce&#123; C: make(chan struct&#123;&#125;), &#125; reloadReady.Close = func() &#123; reloadReady.once.Do(func() &#123; close(reloadReady.C) &#125;) &#125; 根据上面的总结，Termination handler、Scrape discovery manager、Notify discovery manager、TSDB、Web handler组件直接运行，TSDB初始化结束后，关闭dbOpen channel，接着Initial configuration loading结束阻塞，开始运行，在某处关闭了reloadReady.C channel后，等待该channel通知的组件开始运行。 Initial configuration loading组件的execute函数如下，可见最重要的就是reloadConfig函数的实现 select &#123; case &lt;-dbOpen: // In case a shutdown is initiated before the dbOpen is released case &lt;-cancel: reloadReady.Close() return nil &#125; if err := reloadConfig(cfg.configFile, logger, noStepSubqueryInterval, reloaders...); err != nil &#123; return errors.Wrapf(err, &quot;error loading config from %q&quot;, cfg.configFile) &#125; reloadReady.Close() reloadConfig函数实现如下 func reloadConfig(filename string, logger log.Logger, noStepSuqueryInterval *safePromQLNoStepSubqueryInterval, rls ...reloader) (err error) &#123; start := time.Now() timings := []interface&#123;&#125;&#123;&#125; level.Info(logger).Log(&quot;msg&quot;, &quot;Loading configuration file&quot;, &quot;filename&quot;, filename) defer func() &#123; if err == nil &#123; configSuccess.Set(1) configSuccessTime.SetToCurrentTime() &#125; else &#123; configSuccess.Set(0) &#125; &#125;() conf, err := config.LoadFile(filename) if err != nil &#123; return errors.Wrapf(err, &quot;couldn&#39;t load configuration (--config.file=%q)&quot;, filename) &#125; failed := false for _, rl := range rls &#123; rstart := time.Now() if err := rl.reloader(conf); err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, &quot;Failed to apply configuration&quot;, &quot;err&quot;, err) failed = true &#125; timings = append(timings, rl.name, time.Since(rstart)) &#125; if failed &#123; return errors.Errorf(&quot;one or more errors occurred while applying the new configuration (--config.file=%q)&quot;, filename) &#125; noStepSuqueryInterval.Set(conf.GlobalConfig.EvaluationInterval) l := []interface&#123;&#125;&#123;&quot;msg&quot;, &quot;Completed loading of configuration file&quot;, &quot;filename&quot;, filename, &quot;totalDuration&quot;, time.Since(start)&#125; level.Info(logger).Log(append(l, timings...)...) return nil &#125; 该函数里面有两个重要功能，一个是解析配置文件，另一个是调用reloader。reloader的调用先不深究，我们留到下一篇讲scrape的时候再研究，这里先研究一下配置文件的解析，这个也是相当精彩的。配置文件的struct定义如下， type Config struct &#123; GlobalConfig GlobalConfig `yaml:&quot;global&quot;` AlertingConfig AlertingConfig `yaml:&quot;alerting,omitempty&quot;` RuleFiles []string `yaml:&quot;rule_files,omitempty&quot;` ScrapeConfigs []*ScrapeConfig `yaml:&quot;scrape_configs,omitempty&quot;` RemoteWriteConfigs []*RemoteWriteConfig `yaml:&quot;remote_write,omitempty&quot;` RemoteReadConfigs []*RemoteReadConfig `yaml:&quot;remote_read,omitempty&quot;` &#125; 因为需要支持各种动态发现组件，ServiceDiscoveryConfigs在这里tag key为“-”，表示不解析，config.go自定义实现了UnmarshalYAML，用于解析配置 type ScrapeConfig struct &#123; // The job name to which the job label is set by default. JobName string `yaml:&quot;job_name&quot;` // Indicator whether the scraped metrics should remain unmodified. HonorLabels bool `yaml:&quot;honor_labels,omitempty&quot;` // Indicator whether the scraped timestamps should be respected. HonorTimestamps bool `yaml:&quot;honor_timestamps&quot;` // A set of query parameters with which the target is scraped. Params url.Values `yaml:&quot;params,omitempty&quot;` // How frequently to scrape the targets of this scrape config. ScrapeInterval model.Duration `yaml:&quot;scrape_interval,omitempty&quot;` // The timeout for scraping targets of this config. ScrapeTimeout model.Duration `yaml:&quot;scrape_timeout,omitempty&quot;` // The HTTP resource path on which to fetch metrics from targets. MetricsPath string `yaml:&quot;metrics_path,omitempty&quot;` // The URL scheme with which to fetch metrics from targets. Scheme string `yaml:&quot;scheme,omitempty&quot;` // More than this many samples post metric-relabeling will cause the scrape to fail. SampleLimit uint `yaml:&quot;sample_limit,omitempty&quot;` // More than this many targets after the target relabeling will cause the // scrapes to fail. TargetLimit uint `yaml:&quot;target_limit,omitempty&quot;` // We cannot do proper Go type embedding below as the parser will then parse // values arbitrarily into the overflow maps of further-down types. ServiceDiscoveryConfigs discovery.Configs `yaml:&quot;-&quot;` HTTPClientConfig config.HTTPClientConfig `yaml:&quot;,inline&quot;` // List of target relabel configurations. RelabelConfigs []*relabel.Config `yaml:&quot;relabel_configs,omitempty&quot;` // List of metric relabel configurations. MetricRelabelConfigs []*relabel.Config `yaml:&quot;metric_relabel_configs,omitempty&quot;` &#125; conf, err := config.LoadFile(filename)这个调用返回一个Config指针， func Load(s string) (*Config, error) &#123; cfg := &amp;Config&#123;&#125; // If the entire config body is empty the UnmarshalYAML method is // never called. We thus have to set the DefaultConfig at the entry // point as well. *cfg = DefaultConfig err := yaml.UnmarshalStrict([]byte(s), cfg) if err != nil &#123; return nil, err &#125; return cfg, nil &#125; err := yaml.UnmarshalStrict([]byte(s), cfg)调用Config的yaml Unmarshal方法，解析配置，但是Prometheus自定义了整个配置文件的解析，Config的UnmarshalYAML实现如下： func (c *Config) UnmarshalYAML(unmarshal func(interface&#123;&#125;) error) error &#123; *c = DefaultConfig // We want to set c to the defaults and then overwrite it with the input. // To make unmarshal fill the plain data struct rather than calling UnmarshalYAML // again, we have to hide it using a type indirection. type plain Config if err := unmarshal((*plain)(c)); err != nil &#123; return err &#125; // If a global block was open but empty the default global config is overwritten. // We have to restore it here. if c.GlobalConfig.isZero() &#123; c.GlobalConfig = DefaultGlobalConfig &#125; for _, rf := range c.RuleFiles &#123; if !patRulePath.MatchString(rf) &#123; return errors.Errorf(&quot;invalid rule file path %q&quot;, rf) &#125; &#125; // Do global overrides and validate unique names. jobNames := map[string]struct&#123;&#125;&#123;&#125; for _, scfg := range c.ScrapeConfigs &#123; if scfg == nil &#123; return errors.New(&quot;empty or null scrape config section&quot;) &#125; // First set the correct scrape interval, then check that the timeout // (inferred or explicit) is not greater than that. if scfg.ScrapeInterval == 0 &#123; scfg.ScrapeInterval = c.GlobalConfig.ScrapeInterval &#125; if scfg.ScrapeTimeout &gt; scfg.ScrapeInterval &#123; return errors.Errorf(&quot;scrape timeout greater than scrape interval for scrape config with job name %q&quot;, scfg.JobName) &#125; if scfg.ScrapeTimeout == 0 &#123; if c.GlobalConfig.ScrapeTimeout &gt; scfg.ScrapeInterval &#123; scfg.ScrapeTimeout = scfg.ScrapeInterval &#125; else &#123; scfg.ScrapeTimeout = c.GlobalConfig.ScrapeTimeout &#125; &#125; if _, ok := jobNames[scfg.JobName]; ok &#123; return errors.Errorf(&quot;found multiple scrape configs with job name %q&quot;, scfg.JobName) &#125; jobNames[scfg.JobName] = struct&#123;&#125;&#123;&#125; &#125; rwNames := map[string]struct&#123;&#125;&#123;&#125; for _, rwcfg := range c.RemoteWriteConfigs &#123; if rwcfg == nil &#123; return errors.New(&quot;empty or null remote write config section&quot;) &#125; // Skip empty names, we fill their name with their config hash in remote write code. if _, ok := rwNames[rwcfg.Name]; ok &amp;&amp; rwcfg.Name != &quot;&quot; &#123; return errors.Errorf(&quot;found multiple remote write configs with job name %q&quot;, rwcfg.Name) &#125; rwNames[rwcfg.Name] = struct&#123;&#125;&#123;&#125; &#125; rrNames := map[string]struct&#123;&#125;&#123;&#125; for _, rrcfg := range c.RemoteReadConfigs &#123; if rrcfg == nil &#123; return errors.New(&quot;empty or null remote read config section&quot;) &#125; // Skip empty names, we fill their name with their config hash in remote read code. if _, ok := rrNames[rrcfg.Name]; ok &amp;&amp; rrcfg.Name != &quot;&quot; &#123; return errors.Errorf(&quot;found multiple remote read configs with job name %q&quot;, rrcfg.Name) &#125; rrNames[rrcfg.Name] = struct&#123;&#125;&#123;&#125; &#125; return nil &#125; 这个配置的解析和验证实在是太复杂了，我们不管其他struct field的unmarshal，关注scrape configs的解析， func (c *ScrapeConfig) UnmarshalYAML(unmarshal func(interface&#123;&#125;) error) error &#123; *c = DefaultScrapeConfig if err := discovery.UnmarshalYAMLWithInlineConfigs(c, unmarshal); err != nil &#123; return err &#125; if len(c.JobName) == 0 &#123; return errors.New(&quot;job_name is empty&quot;) &#125; // The UnmarshalYAML method of HTTPClientConfig is not being called because it&#39;s not a pointer. // We cannot make it a pointer as the parser panics for inlined pointer structs. // Thus we just do its validation here. if err := c.HTTPClientConfig.Validate(); err != nil &#123; return err &#125; // Check for users putting URLs in target groups. if len(c.RelabelConfigs) == 0 &#123; if err := checkStaticTargets(c.ServiceDiscoveryConfigs); err != nil &#123; return err &#125; &#125; for _, rlcfg := range c.RelabelConfigs &#123; if rlcfg == nil &#123; return errors.New(&quot;empty or null target relabeling rule in scrape config&quot;) &#125; &#125; for _, rlcfg := range c.MetricRelabelConfigs &#123; if rlcfg == nil &#123; return errors.New(&quot;empty or null metric relabeling rule in scrape config&quot;) &#125; &#125; return nil &#125; err := discovery.UnmarshalYAMLWithInlineConfigs(c, unmarshal)里面是解析动态发现的具体实现， let us drill down： func UnmarshalYAMLWithInlineConfigs(out interface&#123;&#125;, unmarshal func(interface&#123;&#125;) error) error &#123; outVal := reflect.ValueOf(out) if outVal.Kind() != reflect.Ptr &#123; return fmt.Errorf(&quot;discovery: can only unmarshal into a struct pointer: %T&quot;, out) &#125; outVal = outVal.Elem() if outVal.Kind() != reflect.Struct &#123; return fmt.Errorf(&quot;discovery: can only unmarshal into a struct pointer: %T&quot;, out) &#125; outTyp := outVal.Type() cfgTyp := getConfigType(outTyp) cfgPtr := reflect.New(cfgTyp) cfgVal := cfgPtr.Elem() // Copy shared fields (defaults) to dynamic value. var configs *Configs for i, n := 0, outVal.NumField(); i &lt; n; i++ &#123; if outTyp.Field(i).Type == configsType &#123; configs = outVal.Field(i).Addr().Interface().(*Configs) continue &#125; if cfgTyp.Field(i).PkgPath != &quot;&quot; &#123; continue // Field is unexported: ignore. &#125; cfgVal.Field(i).Set(outVal.Field(i)) &#125; if configs == nil &#123; return fmt.Errorf(&quot;discovery: Configs field not found in type: %T&quot;, out) &#125; // Unmarshal into dynamic value. if err := unmarshal(cfgPtr.Interface()); err != nil &#123; return replaceYAMLTypeError(err, cfgTyp, outTyp) &#125; // Copy shared fields from dynamic value. for i, n := 0, outVal.NumField(); i &lt; n; i++ &#123; if cfgTyp.Field(i).PkgPath != &quot;&quot; &#123; continue // Field is unexported: ignore. &#125; outVal.Field(i).Set(cfgVal.Field(i)) &#125; var err error *configs, err = readConfigs(cfgVal, outVal.NumField()) return err &#125; Oh, my god! 这个太刺激了，用reflect去实现底层的数据解析，这个以后要看看底层的实现，先不管了，我们看到上面函数的倒数第二句有这样一个调用*configs, err = readConfigs(cfgVal, outVal.NumField())，继续drill down func readConfigs(structVal reflect.Value, startField int) (Configs, error) &#123; var ( configs Configs targets []*targetgroup.Group ) for i, n := startField, structVal.NumField(); i &lt; n; i++ &#123; field := structVal.Field(i) if field.Kind() != reflect.Slice &#123; panic(&quot;discovery: internal error: field is not a slice&quot;) &#125; for k := 0; k &lt; field.Len(); k++ &#123; val := field.Index(k) if val.IsZero() || (val.Kind() == reflect.Ptr &amp;&amp; val.Elem().IsZero()) &#123; key := configFieldNames[field.Type().Elem()] key = strings.TrimPrefix(key, configFieldPrefix) return nil, fmt.Errorf(&quot;empty or null section in %s&quot;, key) &#125; switch c := val.Interface().(type) &#123; case *targetgroup.Group: // Add index to the static config target groups for unique identification // within scrape pool. c.Source = strconv.Itoa(len(targets)) // Coalesce multiple static configs into a single static config. targets = append(targets, c) case Config: configs = append(configs, c) default: panic(&quot;discovery: internal error: slice element is not a Config&quot;) &#125; &#125; &#125; if len(targets) &gt; 0 &#123; configs = append(configs, StaticConfig(targets)) &#125; return configs, nil &#125; 厉害了厉害了，我们终于解析完了scrape config，终于知道可以去抓取哪些目标metrics了。嗯，又有新目标，了解yaml底层解析的原理！！！ 下一篇我们接着研究Prometheus server的scrape实现。","categories":[{"name":"Golang","slug":"Golang","permalink":"https://solaim.github.io/categories/Golang/"}],"tags":[{"name":"Golang, Prometheus, Observability","slug":"Golang-Prometheus-Observability","permalink":"https://solaim.github.io/tags/Golang-Prometheus-Observability/"}]},{"title":"Prometheus内部实现(二)","slug":"Prometheus内部实现-二","date":"2021-03-15T14:46:41.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2021/03/15/Prometheus内部实现-二/","link":"","permalink":"https://solaim.github.io/2021/03/15/Prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0-%E4%BA%8C/","excerpt":"","text":"观察事物，需要提纲挈领！软件的启动是观察软件最好的入口！ Prometheus server实现入口文件在cmd/prometheus/main.go。 Prometheus的配置由两个部分构成： 配置文件：prometheus.yml 启动flag：启动时指定 入口函数如下： func main() &#123; ... &#125; 初始化一个flagConfig，这个配置贯穿了整个程序 cfg := flagConfig&#123; notifier: notifier.Options&#123; Registerer: prometheus.DefaultRegisterer, &#125;, web: web.Options&#123; Registerer: prometheus.DefaultRegisterer, Gatherer: prometheus.DefaultGatherer, &#125;, promlogConfig: promlog.Config&#123;&#125;, &#125; 使用kingpin注册flag a := kingpin.New(filepath.Base(os.Args[0]), &quot;The Prometheus monitoring server&quot;).UsageWriter(os.Stdout) a.Version(version.Print(&quot;prometheus&quot;)) a.HelpFlag.Short(&#39;h&#39;) a.Flag(&quot;config.file&quot;, &quot;Prometheus configuration file path.&quot;). Default(&quot;prometheus.yml&quot;).StringVar(&amp;cfg.configFile) a.Flag(&quot;web.listen-address&quot;, &quot;Address to listen on for UI, API, and telemetry.&quot;). Default(&quot;0.0.0.0:9090&quot;).StringVar(&amp;cfg.web.ListenAddress) ... _, err := a.Parse(os.Args[1:]) if err != nil &#123; fmt.Fprintln(os.Stderr, errors.Wrapf(err, &quot;Error parsing commandline arguments&quot;)) a.Usage(os.Args[1:]) os.Exit(2) &#125; 加载配置文件，检查配置合法性 if _, err := config.LoadFile(cfg.configFile); err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, fmt.Sprintf(&quot;Error loading config (--config.file=%s)&quot;, cfg.configFile), &quot;err&quot;, err) os.Exit(2) &#125; 初始化各种组件 var ( localStorage = &amp;readyStorage&#123;&#125; scraper = &amp;readyScrapeManager&#123;&#125; remoteStorage = remote.NewStorage(log.With(logger, &quot;component&quot;, &quot;remote&quot;), prometheus.DefaultRegisterer, localStorage.StartTime, cfg.localStoragePath, time.Duration(cfg.RemoteFlushDeadline), scraper) fanoutStorage = storage.NewFanout(logger, localStorage, remoteStorage) ) var ( ctxWeb, cancelWeb = context.WithCancel(context.Background()) ctxRule = context.Background() notifierManager = notifier.NewManager(&amp;cfg.notifier, log.With(logger, &quot;component&quot;, &quot;notifier&quot;)) ctxScrape, cancelScrape = context.WithCancel(context.Background()) discoveryManagerScrape = discovery.NewManager(ctxScrape, log.With(logger, &quot;component&quot;, &quot;discovery manager scrape&quot;), discovery.Name(&quot;scrape&quot;)) ctxNotify, cancelNotify = context.WithCancel(context.Background()) discoveryManagerNotify = discovery.NewManager(ctxNotify, log.With(logger, &quot;component&quot;, &quot;discovery manager notify&quot;), discovery.Name(&quot;notify&quot;)) scrapeManager = scrape.NewManager(log.With(logger, &quot;component&quot;, &quot;scrape manager&quot;), fanoutStorage) opts = promql.EngineOpts&#123; Logger: log.With(logger, &quot;component&quot;, &quot;query engine&quot;), Reg: prometheus.DefaultRegisterer, MaxSamples: cfg.queryMaxSamples, Timeout: time.Duration(cfg.queryTimeout), ActiveQueryTracker: promql.NewActiveQueryTracker(cfg.localStoragePath, cfg.queryConcurrency, log.With(logger, &quot;component&quot;, &quot;activeQueryTracker&quot;)), LookbackDelta: time.Duration(cfg.lookbackDelta), NoStepSubqueryIntervalFn: noStepSubqueryInterval.Get, EnableAtModifier: cfg.enablePromQLAtModifier, EnableNegativeOffset: cfg.enablePromQLNegativeOffset, &#125; queryEngine = promql.NewEngine(opts) ruleManager = rules.NewManager(&amp;rules.ManagerOptions&#123; Appendable: fanoutStorage, Queryable: localStorage, QueryFunc: rules.EngineQueryFunc(queryEngine, fanoutStorage), NotifyFunc: sendAlerts(notifierManager, cfg.web.ExternalURL.String()), Context: ctxRule, ExternalURL: cfg.web.ExternalURL, Registerer: prometheus.DefaultRegisterer, Logger: log.With(logger, &quot;component&quot;, &quot;rule manager&quot;), OutageTolerance: time.Duration(cfg.outageTolerance), ForGracePeriod: time.Duration(cfg.forGracePeriod), ResendDelay: time.Duration(cfg.resendDelay), &#125;) ) scraper.Set(scrapeManager) 将配置更新到flagConfig cfg.web.Context = ctxWeb cfg.web.TSDBRetentionDuration = cfg.tsdb.RetentionDuration cfg.web.TSDBMaxBytes = cfg.tsdb.MaxBytes cfg.web.TSDBDir = cfg.localStoragePath cfg.web.LocalStorage = localStorage cfg.web.Storage = fanoutStorage cfg.web.QueryEngine = queryEngine cfg.web.ScrapeManager = scrapeManager cfg.web.RuleManager = ruleManager cfg.web.Notifier = notifierManager cfg.web.LookbackDelta = time.Duration(cfg.lookbackDelta) cfg.web.Version = &amp;web.PrometheusVersion&#123; Version: version.Version, Revision: version.Revision, Branch: version.Branch, BuildUser: version.BuildUser, BuildDate: version.BuildDate, GoVersion: version.GoVersion, &#125; 注册各种组件的reload函数，main函数的实现精彩之一，结合后面的各种组件启动，非常巧妙的实现了加载操作， reloaders := []reloader&#123; &#123; name: &quot;remote_storage&quot;, reloader: remoteStorage.ApplyConfig, &#125;, &#123; name: &quot;web_handler&quot;, reloader: webHandler.ApplyConfig, &#125;, &#123; name: &quot;query_engine&quot;, reloader: func(cfg *config.Config) error &#123; if cfg.GlobalConfig.QueryLogFile == &quot;&quot; &#123; queryEngine.SetQueryLogger(nil) return nil &#125; l, err := logging.NewJSONFileLogger(cfg.GlobalConfig.QueryLogFile) if err != nil &#123; return err &#125; queryEngine.SetQueryLogger(l) return nil &#125;, &#125;, &#123; // The Scrape and notifier managers need to reload before the Discovery manager as // they need to read the most updated config when receiving the new targets list. name: &quot;scrape&quot;, reloader: scrapeManager.ApplyConfig, &#125;, &#123; name: &quot;scrape_sd&quot;, reloader: func(cfg *config.Config) error &#123; c := make(map[string]discovery.Configs) for _, v := range cfg.ScrapeConfigs &#123; c[v.JobName] = v.ServiceDiscoveryConfigs &#125; return discoveryManagerScrape.ApplyConfig(c) &#125;, &#125;, &#123; name: &quot;notify&quot;, reloader: notifierManager.ApplyConfig, &#125;, &#123; name: &quot;notify_sd&quot;, reloader: func(cfg *config.Config) error &#123; c := make(map[string]discovery.Configs) for k, v := range cfg.AlertingConfig.AlertmanagerConfigs.ToMap() &#123; c[k] = v.ServiceDiscoveryConfigs &#125; return discoveryManagerNotify.ApplyConfig(c) &#125;, &#125;, &#123; name: &quot;rules&quot;, reloader: func(cfg *config.Config) error &#123; // Get all rule files matching the configuration paths. var files []string for _, pat := range cfg.RuleFiles &#123; fs, err := filepath.Glob(pat) if err != nil &#123; // The only error can be a bad pattern. return errors.Wrapf(err, &quot;error retrieving rule files for %s&quot;, pat) &#125; files = append(files, fs...) &#125; return ruleManager.Update( time.Duration(cfg.GlobalConfig.EvaluationInterval), files, cfg.GlobalConfig.ExternalLabels, ) &#125;, &#125;, &#125; 这里声明TSDB、配置加载等的通知channel，用于控制组件启动的顺序 // Start all components while we wait for TSDB to open but only load // initial config and mark ourselves as ready after it completed. dbOpen := make(chan struct&#123;&#125;) // sync.Once is used to make sure we can close the channel at different execution stages(SIGTERM or when the config is loaded). type closeOnce struct &#123; C chan struct&#123;&#125; once sync.Once Close func() &#125; // Wait until the server is ready to handle reloading. reloadReady := &amp;closeOnce&#123; C: make(chan struct&#123;&#125;), &#125; reloadReady.Close = func() &#123; reloadReady.once.Do(func() &#123; close(reloadReady.C) &#125;) &#125; 使用jaeger，进行链路追踪 closer, err := initTracing(logger) if err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, &quot;Unable to init tracing&quot;, &quot;err&quot;, err) os.Exit(2) &#125; defer closer.Close() 设置web listener以及验证web的配置 listener, err := webHandler.Listener() if err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, &quot;Unable to start web listener&quot;, &quot;err&quot;, err) os.Exit(1) &#125; err = toolkit_web.Validate(*webConfig) if err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, &quot;Unable to validate web configuration file&quot;, &quot;err&quot;, err) os.Exit(1) &#125; 然后，来到main函数的高潮，设置各个组件，底层使用了oklog&#x2F;run，便于一组goroutine的控制， var g run.Group &#123; // Termination handler. term := make(chan os.Signal, 1) signal.Notify(term, os.Interrupt, syscall.SIGTERM) cancel := make(chan struct&#123;&#125;) g.Add( func() error &#123; // Don&#39;t forget to release the reloadReady channel so that waiting blocks can exit normally. select &#123; case &lt;-term: level.Warn(logger).Log(&quot;msg&quot;, &quot;Received SIGTERM, exiting gracefully...&quot;) reloadReady.Close() case &lt;-webHandler.Quit(): level.Warn(logger).Log(&quot;msg&quot;, &quot;Received termination request via web service, exiting gracefully...&quot;) case &lt;-cancel: reloadReady.Close() &#125; return nil &#125;, func(err error) &#123; close(cancel) &#125;, ) &#125; &#123; // Scrape discovery manager. g.Add( func() error &#123; err := discoveryManagerScrape.Run() level.Info(logger).Log(&quot;msg&quot;, &quot;Scrape discovery manager stopped&quot;) return err &#125;, func(err error) &#123; level.Info(logger).Log(&quot;msg&quot;, &quot;Stopping scrape discovery manager...&quot;) cancelScrape() &#125;, ) &#125; &#123; // Notify discovery manager. g.Add( func() error &#123; err := discoveryManagerNotify.Run() level.Info(logger).Log(&quot;msg&quot;, &quot;Notify discovery manager stopped&quot;) return err &#125;, func(err error) &#123; level.Info(logger).Log(&quot;msg&quot;, &quot;Stopping notify discovery manager...&quot;) cancelNotify() &#125;, ) &#125; &#123; // Scrape manager. g.Add( func() error &#123; // When the scrape manager receives a new targets list // it needs to read a valid config for each job. // It depends on the config being in sync with the discovery manager so // we wait until the config is fully loaded. &lt;-reloadReady.C err := scrapeManager.Run(discoveryManagerScrape.SyncCh()) level.Info(logger).Log(&quot;msg&quot;, &quot;Scrape manager stopped&quot;) return err &#125;, func(err error) &#123; // Scrape manager needs to be stopped before closing the local TSDB // so that it doesn&#39;t try to write samples to a closed storage. level.Info(logger).Log(&quot;msg&quot;, &quot;Stopping scrape manager...&quot;) scrapeManager.Stop() &#125;, ) &#125; &#123; // Reload handler. // Make sure that sighup handler is registered with a redirect to the channel before the potentially // long and synchronous tsdb init. hup := make(chan os.Signal, 1) signal.Notify(hup, syscall.SIGHUP) cancel := make(chan struct&#123;&#125;) g.Add( func() error &#123; &lt;-reloadReady.C for &#123; select &#123; case &lt;-hup: if err := reloadConfig(cfg.configFile, logger, noStepSubqueryInterval, reloaders...); err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, &quot;Error reloading config&quot;, &quot;err&quot;, err) &#125; case rc := &lt;-webHandler.Reload(): if err := reloadConfig(cfg.configFile, logger, noStepSubqueryInterval, reloaders...); err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, &quot;Error reloading config&quot;, &quot;err&quot;, err) rc &lt;- err &#125; else &#123; rc &lt;- nil &#125; case &lt;-cancel: return nil &#125; &#125; &#125;, func(err error) &#123; // Wait for any in-progress reloads to complete to avoid // reloading things after they have been shutdown. cancel &lt;- struct&#123;&#125;&#123;&#125; &#125;, ) &#125; &#123; // Initial configuration loading. cancel := make(chan struct&#123;&#125;) g.Add( func() error &#123; select &#123; case &lt;-dbOpen: // In case a shutdown is initiated before the dbOpen is released case &lt;-cancel: reloadReady.Close() return nil &#125; if err := reloadConfig(cfg.configFile, logger, noStepSubqueryInterval, reloaders...); err != nil &#123; return errors.Wrapf(err, &quot;error loading config from %q&quot;, cfg.configFile) &#125; reloadReady.Close() webHandler.Ready() level.Info(logger).Log(&quot;msg&quot;, &quot;Server is ready to receive web requests.&quot;) &lt;-cancel return nil &#125;, func(err error) &#123; close(cancel) &#125;, ) &#125; &#123; // Rule manager. g.Add( func() error &#123; &lt;-reloadReady.C ruleManager.Run() return nil &#125;, func(err error) &#123; ruleManager.Stop() &#125;, ) &#125; &#123; // TSDB. opts := cfg.tsdb.ToTSDBOptions() cancel := make(chan struct&#123;&#125;) g.Add( func() error &#123; level.Info(logger).Log(&quot;msg&quot;, &quot;Starting TSDB ...&quot;) if cfg.tsdb.WALSegmentSize != 0 &#123; if cfg.tsdb.WALSegmentSize &lt; 10*1024*1024 || cfg.tsdb.WALSegmentSize &gt; 256*1024*1024 &#123; return errors.New(&quot;flag &#39;storage.tsdb.wal-segment-size&#39; must be set between 10MB and 256MB&quot;) &#125; &#125; db, err := openDBWithMetrics( cfg.localStoragePath, logger, prometheus.DefaultRegisterer, &amp;opts, ) if err != nil &#123; return errors.Wrapf(err, &quot;opening storage failed&quot;) &#125; switch fsType := prom_runtime.Statfs(cfg.localStoragePath); fsType &#123; case &quot;NFS_SUPER_MAGIC&quot;: level.Warn(logger).Log(&quot;fs_type&quot;, fsType, &quot;msg&quot;, &quot;This filesystem is not supported and may lead to data corruption and data loss. Please carefully read https://prometheus.io/docs/prometheus/latest/storage/ to learn more about supported filesystems.&quot;) default: level.Info(logger).Log(&quot;fs_type&quot;, fsType) &#125; level.Info(logger).Log(&quot;msg&quot;, &quot;TSDB started&quot;) level.Debug(logger).Log(&quot;msg&quot;, &quot;TSDB options&quot;, &quot;MinBlockDuration&quot;, cfg.tsdb.MinBlockDuration, &quot;MaxBlockDuration&quot;, cfg.tsdb.MaxBlockDuration, &quot;MaxBytes&quot;, cfg.tsdb.MaxBytes, &quot;NoLockfile&quot;, cfg.tsdb.NoLockfile, &quot;RetentionDuration&quot;, cfg.tsdb.RetentionDuration, &quot;WALSegmentSize&quot;, cfg.tsdb.WALSegmentSize, &quot;AllowOverlappingBlocks&quot;, cfg.tsdb.AllowOverlappingBlocks, &quot;WALCompression&quot;, cfg.tsdb.WALCompression, ) startTimeMargin := int64(2 * time.Duration(cfg.tsdb.MinBlockDuration).Seconds() * 1000) localStorage.Set(db, startTimeMargin) close(dbOpen) &lt;-cancel return nil &#125;, func(err error) &#123; if err := fanoutStorage.Close(); err != nil &#123; level.Error(logger).Log(&quot;msg&quot;, &quot;Error stopping storage&quot;, &quot;err&quot;, err) &#125; close(cancel) &#125;, ) &#125; &#123; // Web handler. g.Add( func() error &#123; if err := webHandler.Run(ctxWeb, listener, *webConfig); err != nil &#123; return errors.Wrapf(err, &quot;error starting web server&quot;) &#125; return nil &#125;, func(err error) &#123; cancelWeb() &#125;, ) &#125; &#123; // Notifier. // Calling notifier.Stop() before ruleManager.Stop() will cause a panic if the ruleManager isn&#39;t running, // so keep this interrupt after the ruleManager.Stop(). g.Add( func() error &#123; // When the notifier manager receives a new targets list // it needs to read a valid config for each job. // It depends on the config being in sync with the discovery manager // so we wait until the config is fully loaded. &lt;-reloadReady.C notifierManager.Run(discoveryManagerNotify.SyncCh()) level.Info(logger).Log(&quot;msg&quot;, &quot;Notifier manager stopped&quot;) return nil &#125;, func(err error) &#123; notifierManager.Stop() &#125;, ) &#125; 最后启动g，整体程序就运行起来了，g.Run正常运行期间不会返回，一旦上面注册的一个goroutine失败，整体退出 if err := g.Run(); err != nil &#123; level.Error(logger).Log(&quot;err&quot;, err) os.Exit(1) &#125; level.Info(logger).Log(&quot;msg&quot;, &quot;See you next time!&quot;) 下一篇我们开始研究Prometheus server的Initial configuration loading代码。","categories":[{"name":"Golang","slug":"Golang","permalink":"https://solaim.github.io/categories/Golang/"}],"tags":[{"name":"Golang, Prometheus, Observability","slug":"Golang-Prometheus-Observability","permalink":"https://solaim.github.io/tags/Golang-Prometheus-Observability/"}]},{"title":"Prometheus内部实现(一)","slug":"Prometheus内部实现-一","date":"2021-03-14T10:21:00.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2021/03/14/Prometheus内部实现-一/","link":"","permalink":"https://solaim.github.io/2021/03/14/Prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0-%E4%B8%80/","excerpt":"","text":"Prometheus在监控界为什么这么🔥？ Prometheus具体是怎么实现的？ 人如其名，产品亦是！Prometheus就是火种，照耀在当前的监控世界！ Prometheus这把火，在我看来，火在三个方面： 系出名门：CNCF的第二个项目，CNCF的老大叫K8S，老大对老二很照顾，监控首选。 门庭若市：开源，大厂竞相争用。 简单易用：架构简单，配置简单，上下游打通。 大家都在用，而且用起来很方便的确是我们选择产品的方法，但是作为开发人员，还是需要了解一下其内部的实现。这个系列，主要根据Prometheus的代码和文档，梳理Prometheus的内部实现。 总体架构 Prometheus从配置的jobs中抓取metrics，存储在本地存储或者外部存储中，在metrics上运行规则聚合多维度的数据或者生成告警。Grafana或者自带的web UI能够方便的展示数据。 Prometheus的组件包括： server：负责抓取metrics，存储metrics，分析、聚合metrics client library：多种语言的客户端库，方便实现 push gateway：短期存活服务metrics推送网关 exporters：比如监控节点的node_exporter，监控MySQL的mysqld_expoter等等 alertmanager：alertmanager负责告警的发送 从总体架构，我们可以了解各种组件，以及数据的流向，现在我们开始研究Prometheus server的内部实现。 Server实现架构 server启动的时候，根据配置文件prometheus.yml和flag决定各种配置，配置文件包括全局配置、抓取目标配置和规则配置等等。scrape_configs配置了抓取的目标，Prometheus支持动态获取目标，Service discovery读取静态配置的目标或者周期更新动态目标，添加到scrape manager，scrape manager负责抓取目标，抓取实际是一个HTTP请求，即各种exporters实际上就是一个简单的HTTP服务，负责暴露各种应用的指标。scrape manager抓取指标后，将数据交给fanout storage，fanout storage负责数据存储，支持本地存储和外部存储。rule manager根据规则，聚合指标存储到fanout storage，或者生成告警发送给alert manager。Prometheus也实现了各种web接口，使用promQL进行数据的查询。 server的组件包括： Termination handler. Scrape discovery manager. Notify discovery manager. Scrape manager. Reload handler. Rule manager. TSDB. Web handler. Notifier. 了解了server的基本架构，下面开始研究server的配置。 Prometheus.ymlglobal: scrape_interval: 15s evaluation_interval: 15s rule_files: # - &quot;first.rules&quot; # - &quot;second.rules&quot; scrape_configs: - job_name: prometheus static_configs: - targets: [&#39;localhost:9090&#39;] 这是官网提供的一个简单的配置文件，包含了三个部分：全局配置、规则配置和抓取配置。global包括全局的配置，rule_files配置规则文件，这里没有配置，scrape_configs配置抓取目标。抓取目标是一个列表，包括多个job，每个job有一个job_name，目标配置以及其他配置，目标配置因为支持多种目标源，所以这块在代码中的实现比较复杂。 下一篇我们开始研究Prometheus server的启动源代码。","categories":[{"name":"Golang","slug":"Golang","permalink":"https://solaim.github.io/categories/Golang/"}],"tags":[{"name":"Golang, Prometheus, Observability","slug":"Golang-Prometheus-Observability","permalink":"https://solaim.github.io/tags/Golang-Prometheus-Observability/"}]},{"title":"k8s-yaml配置文件","slug":"k8s-yaml配置文件","date":"2019-11-18T14:26:39.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2019/11/18/k8s-yaml配置文件/","link":"","permalink":"https://solaim.github.io/2019/11/18/k8s-yaml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","excerpt":"","text":"1、Redis Deployment部署示例yaml先来看一个部署yaml示例文件： # application/guestbook/redis-master-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-master labels: app: redis spec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: labels: app: redis role: master tier: backend spec: containers: - name: master image: redis # or just image: redis resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 通过执行命令kubectl create -f redis-master-deployment.yaml，可以创建1个Redis master Pod。如果我想通过yaml文件创建3个Pod，怎么办？ 很简单，只需要将yaml文件中的spec.replicas设置为3即可，那么一个yaml文件可以控制什么呢？ 我们通过四个模块简单的介绍一下k8s使用的yaml配置文件，当然了也可以使用json格式，毕竟kubectl是把yaml文件转换成json文件之后再发送给kube-apiServer的。 2、apiVersion模块为啥要在这引入一个apiVersion？apiVersion跟软件版本有没有关系？… 一个简单的命令，能够引出很多问题，但是回答问题，需要一个一个来。 首先，k8s是master-node架构模式，master上一般不运行kubelet（要是机器牛逼也可以的哈），只负责资源的管理和调度等等工作。我们使用kubectl的时候，实际是在跟master节点上的kube-apiServer进行通信，所有的请求提交到kube-apiServer，而kubectl跟kube-apiServer的通信实际上是一次http请求，我们使用curl命令可以进行简单的测试就可以证实这一点。 其次，k8s整体版本已经升级到1.16，版本升级带来可以操控资源的变动，开发者期待增加新的资源，以面对不同场景的需求，但是已经存在的客户端怎么办了，武断的直接升级，会破坏兼容性，所以，引入apiVersion可以保证兼容性。 所以使用apiVersion既可以保证兼容性，又可以兼顾实现层面的问题。So，next one！ apiVersion跟软件的Version是没有直接关系的，apiVersion可能存在好几个版本中。不同级别的apiVersion，意味着不同的稳定性和支持度。apiVersion的级别如下： 序号 版本 备注 1 alpha 名称中包含alpha，可能存在bug，版本特性可能被丢弃，后续版本可能不兼容，测试环境使用 2 beta 名称中包含beta，代码经过测试，版本特性不会丢弃，实现层面可能变动，非商业环境使用 3 stable 名称中包含vX，X是一个整数 为了扩展k8s api方便，开发人员实现了API groups。API groups在rest路径中指定，而在apiVersion中是一个序列化的对象。目前使用的API groups包含： 1、核心组：rest路径是&#x2F;api&#x2F;v1，apiVersion中是v1 2、命名组：rest路径是&#x2F;apis&#x2F;$GROUP_NAME&#x2F;$VERSION,apiVersion中是$GROUP_NAME&#x2F;$VERSION 有老铁可能发现了，你这示例给的apps&#x2F;v1,apps是啥了？apps就是一个命名组的名称，表示功能和k8s运行的应用有关联，比如说Deployments, RollingUpdates, and ReplicaSets。当然同一个路径下可能包含多种类型的资源，所以k8s为了区别具体的区别，使用了kind标识具体的资源类型。 3、kind模块kind字段用于标识资源类型，那么k8s支持的资源类型有哪些了？ 太多了，不一一列举了。提几个常用的，比如Deployment、ReplicaController、Pod和Service等等。现在我们创建了一个对象，我们想给她一个名字，怎么办？ 4、metadata模块显示，metadata模块可以满足我们的这个需求，metadata.name就是描述我们刚刚创建的对象的名称，当然k8s还有其他的metadata需要描述，比如metadata.namespace，默认为default，metadata.uid不需要手动填写，系统自动生成。这几个要么必须填写，要么系统生成，是必备硬货。为了方便用户组织资源，metadata提供labels字段，可以组织和分类对象。label是一个键值对，labels可以同时又多个label，逗号分隔。还有其他字段，就不细说了。 好，上面三个模块基本描述了我要创建一个什么样的对象，那具体咋实现了？假设我们有两个时间点，当前时间和最终时间。我们需要一个模块去描述最终时间点资源对象的状态，这就是spec模块的活。 5、spec模块额，这个模块就是干脏活、累活的。 创建的对象牛不牛逼，就看在这咋写了。 既然spec是写最终时间点资源对象的状态，我们可以这样描述：我想要3个黄色的、标准大小的红双喜乒乓球。这里乒乓球我们要红双喜的，数量是三个，性质是黄色的、标准大小的。所以spec.selector就是选择哪个牌子的乒乓球，spec.replicas是乒乓球的数量，spec.template则描述乒乓球的具体属性信息。看来template还是实现最终状态的关键，模板template就是用来制作红双喜黄色、标准大小乒乓球的。每个被制造的乒乓球都具有相同的元信息、制作过程和功能，k8s用spec.template.metadata描述元信息，spec.template.spec描述制作过程。在k8s的底层使用的是容器技术，所以spec.template.spec实际描述怎么创建一个容器，如果kind是Service，是没有spec.template.spec字段的。当然，spec下面还有其他字段，需要根据apiVersion和kind进行适配，这里就不细说了。 所以，yaml文件描述了如何创建一个资源对象。apiVersion指定了创建资源对象使用的k8s API版本，kind指定了类型，metadata唯一的描述了资源对象，spec描述了最终时间点资源对象的状态。 嗯，看完这个，是可以照猫画虎了，但还是有点懵。为了不再懵圈，接下来，是时候好好研究研究k8s的架构和底层实现了。下次见！！！","categories":[{"name":"k8s","slug":"k8s","permalink":"https://solaim.github.io/categories/k8s/"}],"tags":[]},{"title":"k8s官网留言板demo","slug":"k8s官网留言板demo","date":"2019-10-20T10:28:40.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2019/10/20/k8s官网留言板demo/","link":"","permalink":"https://solaim.github.io/2019/10/20/k8s%E5%AE%98%E7%BD%91%E7%95%99%E8%A8%80%E6%9D%BFdemo/","excerpt":"","text":"https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#start-up-the-redis-master 1、部署redis-master Deployment Step1 编辑redis-master-deployment.yaml # application/guestbook/redis-master-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-master labels: app: redis spec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: labels: app: redis role: master tier: backend spec: containers: - name: master image: redis # or just image: redis resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Step2 执行命令 # 创建Deployment kubectl create -f redis-master-deployment.yaml # 查看pods kubectl get pods 2、部署redis-master Service Step1 编辑redis-master-service.yaml # application/guestbook/redis-master-service.yaml apiVersion: v1 kind: Service metadata: name: redis-master labels: app: redis role: master tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: master tier: backend Step2 执行命令 # 创建DService kubectl create -f redis-master-service.yaml # 查看service kubectl get services 3、部署redis-slave Deployment Step1 编辑redis-slave-deployment.yaml # application/guestbook/redis-slave-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-slave labels: app: redis spec: selector: matchLabels: app: redis role: slave tier: backend replicas: 2 template: metadata: labels: app: redis role: slave tier: backend spec: containers: - name: slave image: gcr.io/google_samples/gb-redisslave:v3 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service&#39;s host. To do so, comment out the &#39;value: dns&#39; line above, and # uncomment the line below: # value: env ports: - containerPort: 6379 Step2 执行命令 kubectl create -f redis-slave-deployment.yaml kubectl get pods 4、部署redis slave Service Step1 编辑redis-slave-service.yaml # application/guestbook/redis-slave-service.yaml apiVersion: v1 kind: Service metadata: name: redis-slave labels: app: redis role: slave tier: backend spec: ports: - port: 6379 selector: app: redis role: slave tier: backend Step2 执行命令 kubectl create -f redis-slave-service.yaml kubectl get services 5、部署guestbook Deployment Step1 编辑frontend-deployment.yaml # application/guestbook/frontend-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: frontend labels: app: guestbook spec: selector: matchLabels: app: guestbook tier: frontend replicas: 3 template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google-samples/gb-frontend:v6 # 官网上使用的是v4版，在使用中出现ImageInspectErr，镜像出现问题，升级为v6 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service&#39;s host. To do so, comment out the &#39;value: dns&#39; line above, and # uncomment the line below: # value: env ports: - containerPort: 80 Step2 执行命令 kubectl create -f frontend-deployment.yaml kubectl get pods 6、部署guestbook Service Step1 编辑frontend-service.yaml # application/guestbook/frontend-service.yaml apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # comment or delete the following line if you want to use a LoadBalancer type: NodePort # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer ports: - port: 80 selector: app: guestbook tier: frontend Step2 执行命令 kubectl create -f frontend-service.yaml kubectl get services 然后使用node的IP和端口，就可以访问服务了。 7、清理kubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment -l app=guestbook kubectl delete service -l app=guestbook 8、小结通过部署一个留言板服务，熟悉了kubernetes的基本的使用，知道一个服务怎么通过kubernetes部署起来。服务虽然部署起来了，但是kubernetes是怎么实现的呢？它都帮我们做了什么呢？不过在搞清楚这些之前，我们下一篇文章先说说kuberneters使用的yaml配置文件。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://solaim.github.io/categories/k8s/"}],"tags":[]},{"title":"k8s集群搭建","slug":"k8s集群搭建","date":"2019-10-15T12:59:28.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2019/10/15/k8s集群搭建/","link":"","permalink":"https://solaim.github.io/2019/10/15/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","excerpt":"","text":"1、节点 节点 IP master 192.168.124.100 node1 192.168.124.101 node2 192.168.124.102 node3 192.168.124.103 2、docker安装安装使用官方源，安装命令如下： # 下载仓库 cd /etc/yum.repos.d wget https://download.docker.com/linux/centos/docker-ce.repo yum makecache fast # 安装docker yum install docker-ce docker-ce-cli containerd.io # 启动docker systemctl enable docker &amp;&amp; systemctl start docker 3、kubernetes安装# 所有节点执行 # 添加仓库 cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 更新yum cache yum makecache fast # kubeadm初始化会报错, 需要执行下列命令 # 1、关闭firewalld systemctl diable firewalld &amp;&amp; systemctl stop firewalld # 2、关闭selinux setenforce 0 &amp;&amp; sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config # 3、关闭swap swapoff -a &amp;&amp; sed -i &#39;/ swap / s/^/#/&#39; /etc/fstab # 4、设置网络 touch /etc/sysctl.d/k8s.conf cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF modprobe br_netfilter &amp;&amp; sysctl -p /etc/sysctl.d/k8s.conf # master节点执行 yum install kubeadm kubectl kubelet # 所有node节点执行 yum install kubeadm kubelet # 启动实际运行容器的服务kubelet systemctl enable kubelet &amp;&amp; systemctl start kubelet # 现在执行kubelet会报错，不用管 # kubeadm init或join之后，systemd会自动拉起kubelet # kubeadm是集群管理工具 # kubelet负责运行pods # kubectl是客户端，负责和kube-apiserver通信 # 用类比解释的话，kubectl是curl，kube-apiserver是服务器 # kube-controller-manager是控制器 # kube-scheduler是调度器 # kube-proxy负责网络通信 # pause是pod空闲运行的镜像 # etcd用于分布式存储集群信息 # 由于kubeadm使用镜像拉起k8s，但是被墙了，所以在可以翻墙的主机上下载镜像，然后使用下列命令导出镜像 # docker save -o package.tar.gz image1:latest # 使用下面命令导入镜像 # docker load -i package.tar.gz # 获取需要下载的镜像列表 kubeadm config images list # 输出如下，master表示在主节点安装，node表示在从节点安装 k8s.gcr.io/kube-apiserver:v1.16.1 # master k8s.gcr.io/kube-controller-manager:v1.16.1 # master k8s.gcr.io/kube-scheduler:v1.16.1 # master k8s.gcr.io/kube-proxy:v1.16.1 # master node k8s.gcr.io/pause:3.1 # node k8s.gcr.io/etcd:3.3.15-0 # master k8s.gcr.io/coredns:1.6.2 # master node # 很不幸，被墙了 # 用可以翻墙的主机用docker pull拉取镜像，上传本地即可 # 镜像的打包使用save命令 # 镜像的导入使用load命令 # 下载好镜像，导入完毕 # 在master节点执行 kubeadm init --kubernetes-version=v1.16.1 --apiserver-advertise-address=192.168.124.100 --pod-network-cidr=10.244.0.0/16 # 执行成功后，会提示进行如下操作 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 节点加入集群操作 kubeadm join 192.168.124.100:6443 --token ba04v7.y9ki5jgbss8gs0jx \\ --discovery-token-ca-cert-hash sha256:8404de0f262daf02de05aad96415c79c2ff39b91ce3ee28cd8248426166123a9 # k8s的网络模型是扁平化的，即pod之间需要能够直接访问，在谷歌的实现中，已经支持扁平化网络模型 # 但是我们创建的集群，不满足这种网络模型，所以需要借助插件或者进行主机配置之后才能满足 # 刚开始学习建议使用flannel，减少障碍，后期有需要进行调研学习 # 安装flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 在master执行 kubectl get nodes # 输出，暂时配置了两个节点 NAME STATUS ROLES AGE VERSION master Ready master 50m v1.16.1 node1 Ready &lt;none&gt; 27m v1.16.1 # 没有安装flannel之前，查看nodes，会提示STATUS为NotReady。 # 至此，k8s的基本安装就结束了。 # 是不是很简单 # 后续会继续分享k8s的学习","categories":[{"name":"k8s","slug":"k8s","permalink":"https://solaim.github.io/categories/k8s/"}],"tags":[]},{"title":"Flask-SocketIO基本使用","slug":"Flask-SocketIO基本使用","date":"2019-04-11T13:10:04.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2019/04/11/Flask-SocketIO基本使用/","link":"","permalink":"https://solaim.github.io/2019/04/11/Flask-SocketIO%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Flask-SocketIO是一个用于建立WebSocket全双工通信的Flask扩展。客户端应用可以使用任一SocketIO官方客户端库实现，或可以建立持久连接的兼容客户端实现。 1.安装 pip install flask-socketio Flask-SocketIO提供三种异步服务机制： eventlet：支持长轮询和websocket gevent：需使用第三方库 Werkzeug：仅支持长轮询 2.初始化from flask import Flask, render_template from flask_socketio import SocketIO app = Flask(__name__) app.config[&#39;SECRET_KEY&#39;] = &#39;secret!&#39; socketio = SocketIO(app) if __name__ == &#39;__main__&#39;: socketio.run(app) 应用必须给客户端返回一个html页面加载SocketIO库，并且建立连接： &lt;script type=&quot;text/javascript&quot; src=&quot;//cdnjs.cloudflare.com/ajax/libs/socket.io/1.3.6/socket.io.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt; var socket = io.connect(&#39;http://&#39; + document.domain + &#39;:&#39; + location.port); socket.on(&#39;connect&#39;, function() &#123; socket.emit(&#39;my event&#39;, &#123;data: &#39;I\\&#39;m connected!&#39;&#125;); &#125;); &lt;/script&gt; 3.接收消息使用SocketIO时，客户端和服务端以事件的方式接收消息-事件驱动。在客户端，javascript使用回调函数。在服务端，Flask-SocketIO需要注册事件处理函数，类似于路由-视图函数机制。 下面的例子创建服务端无名事件处理函数： @socketio.on(&#39;message&#39;) def handle_message(message): print(&#39;received message: &#39; + message) 上面的例子使用字符串消息，也可以使用json格式的消息： @socketio.on(&#39;json&#39;) def handle_json(json): print(&#39;received json: &#39; + str(json)) 最灵活的事件类型是使用定制事件名称。这些事件的消息数据格式可以是字符串、字节、整数或者json： @socketio.on(&#39;my event&#39;) def handle_my_custom_event(json): print(&#39;received json: &#39; + str(json)) 定制的命名事件支持多参数： @socketio.on(&#39;my event&#39;) def handle_my_custom_event(arg1, arg2, arg3): print(&#39;received args: &#39; + arg1 + arg2 + arg3) 命名事件是最灵活的，因为命名事件消除了包含附加元数据（描述消息类型）的需要。Flask-socketIO也支持SocketIO命名域，SocketIO命名域允许客户端复用socket（多个连接建立在同一物理socket）。 @socketio.on(&#39;my event&#39;, namespace=&#39;/test&#39;) def handle_my_custom_namespace_event(json): print(&#39;received json: &#39; + str(json)) 当SocketIO没有显式指定命名域，全局命名域‘／’将会被使用。 当装饰器语法不太合适时，on_event方法可被使用： def my_function_handler(data): pass socketio.on_event(&#39;my event&#39;, my_function_handler, namespace=&#39;/test&#39;) 客户端可能请求一个承认的回调函数，该函数确认接收发送的消息。处理函数的任何返回值将会作为参数递送给客户端的回调函数： @socketio.on(&#39;my event&#39;) def handle_my_custom_event(json): print(&#39;received json: &#39; + str(json)) return &#39;one&#39;, 2 上例中，客户端回调函数将会被传递两个参数：‘one’和2。 4、发送消息Socket IO事件处理函数可以发送回复消息至连接的客户端，方法包括send()和emit()。 from flask_socketio import send, emit @socketio.on(&#39;message&#39;) def handle_message(message): send(message) @socketio.on(&#39;json&#39;) def handle_json(json): send(json, json=True) @socketio.on(&#39;my event&#39;) def handle_my_custom_event(json): emit(&#39;my response&#39;, json) 当使用命名域时，send和emit默认使用接收消息的命名域。不同的命名域可以通过namespace参数指定。 @socketio.on(&#39;message&#39;) def handle_message(message): send(message, namespace=&#39;/chat&#39;) @socketio.on(&#39;my event&#39;) def handle_my_custom_event(json): emit(&#39;my response&#39;, json, namespace=&#39;/chat&#39;) 当使用多个参数回复一个事件时，发送一个tuple： @socketio.on(&#39;my event&#39;) def handle_my_custom_event(json): emit(&#39;my response&#39;, (&#39;foo&#39;, &#39;bar&#39;, json), namespace=&#39;/chat&#39;) SocketIO支持回调函数，该函数用于证实客户端成功接收了消息： def ack(): print &#39;message was received!&#39; @socketio.on(&#39;my event&#39;) def handle_my_custom_event(json): emit(&#39;my response&#39;, json, callback=ack) 当使用回调函数时，js客户端从接收的消息中获取回调函数，客户端回调函数唤起之后服务端的回调函数相应地被唤起。如歌客户端回调函数具有参数，服务端的回调函数具有相同的参数。 5、广播Flask-SocketIO支持广播，只需在send和emit函数中添加关键字参数broadcast&#x3D;True： @socketio.on(&#39;my event&#39;) def handle_my_custom_event(data): emit(&#39;my response&#39;, data, broadcast=True) 对于服务端主动发起的广播，socketio.send()和socketio.emit()方法可以用来广播消息至所有连接的客户端。 def some_function(): socketio.emit(&#39;some event&#39;, &#123;&#39;data&#39;: 42&#125;) 6、RoomsFlask-SocketIO通过join_room和leave_room函数支持room概念。 from flask_socketio import join_room, leave_room @socketio.on(&#39;join&#39;) def on_join(data): username = data[&#39;username&#39;] room = data[&#39;room&#39;] join_room(room) send(username + &#39; has entered the room.&#39;, room=room) @socketio.on(&#39;leave&#39;) def on_leave(data): username = data[&#39;username&#39;] room = data[&#39;room&#39;] leave_room(room) send(username + &#39; has left the room.&#39;, room=room) send和emit函数接收关键字参数room，将消息发送至连接对应room的客户端。 所有的客户端连接room时用session ID命名，可以使用request.sid获取。 7、连接事件Flask-SocketIO也支持连接和连接断开事件。 @socketio.on(&#39;connect&#39;, namespace=&#39;/chat&#39;) def test_connect(): emit(&#39;my response&#39;, &#123;&#39;data&#39;: &#39;Connected&#39;&#125;) @socketio.on(&#39;disconnect&#39;, namespace=&#39;/chat&#39;) def test_disconnect(): print(&#39;Client disconnected&#39;) 8、基于类的命名域上面描述的是基于装饰器的处理函数，也可以使用基于类的命名域处理函数。 from flask_socketio import Namespace, emit class MyCustomNamespace(Namespace): def on_connect(self): pass def on_disconnect(self): pass def on_my_event(self, data): emit(&#39;my_response&#39;, data) socketio.on_namespace(MyCustomNamespace(&#39;/test&#39;)) 9、错误处理@socketio.on_error() # Handles the default namespace def error_handler(e): pass @socketio.on_error(&#39;/chat&#39;) # handles the &#39;/chat&#39; namespace def error_handler_chat(e): pass @socketio.on_error_default # handles all namespaces without an explicit error handler def default_error_handler(e): pass","categories":[{"name":"Python","slug":"Python","permalink":"https://solaim.github.io/categories/Python/"}],"tags":[{"name":"Python, Flask, SocketIO","slug":"Python-Flask-SocketIO","permalink":"https://solaim.github.io/tags/Python-Flask-SocketIO/"}]},{"title":"最好的告别","slug":"最好的告别","date":"2019-04-11T12:47:43.000Z","updated":"2023-09-10T13:21:16.208Z","comments":true,"path":"2019/04/11/最好的告别/","link":"","permalink":"https://solaim.github.io/2019/04/11/%E6%9C%80%E5%A5%BD%E7%9A%84%E5%91%8A%E5%88%AB/","excerpt":"","text":"印裔美籍作者葛文德医生在《最好的告别》一书中着重思考了在人生最后的阶段，对于身临其境的人，什么是最重要的？必须明白的是现代工业社会的科学技术深刻影响了人类生命的进程。农耕社会中，男性和女性大概十五岁左右结婚，开始生儿育女的生活，战争与疾病治疗的手段使得长寿是奢侈的，长寿的困难造就了长者拥有家庭乃至群落决策者的地位。但是随着时光尾巴的拉长，情况在改变，长寿者很多见，新闻竞相报道寿星。长寿对于绝大对数人是喜闻乐见的，但是当一个社会的老龄化问题突然呈现出来，社会真的做好了迎接这一变化的准备了吗？而显然大多地区的人们没有对改变做出改变。 迟到的改变对于身处其中的人们是一种折磨！ 尽管人类的平均寿命在增长，但是生老病死是自然规律，死亡是符合事物发展的规律的。抽象地说起死亡，这些毫无问题。但是当面对生活周围活生生的家人或者朋友，正在死亡的边缘转圈，他要怎么办？我要怎么办？ “作为会老、会死的高级动物是怎么为自己的生命画上句号的？医学如何改变了死亡体验却又无法改变死亡的牌局？我们关于生命有限性的观念产生了怎样的迷茫？”作者如此问自己。这就是作者的初衷，理清这团乱麻！“应该如何优雅地跨越生命的终点？对此，大多数人缺少清晰的观念，而只是把命运交由医学、技术和陌生人来掌控。” 大工业时代，人类的寿命延长了，但是活的太久了，问题也就来了。现代社会对长者的经验和人情练达的期待明显降低，子女成家后对，不再期待父母对其提供庇护和安全感，当长寿者的年龄超过一个奇点，子女与父母的代际冲突越发明显。“经济全球化戏剧性地改变了年轻人的生存境遇。国家的繁荣有赖于他们逃离家庭期望的束缚，走自己的路-去任何能够找到工作的地方，做任何喜欢的工作，同任何自己喜欢的人结婚。”可以欣喜的是，社会似乎提供了一种两全其美的解决方案，对于年轻人和老年人-独立居住。但是这种解决方案有一个明显的问题：“当独立、自助的生活不能再维持时，我们该怎么办？” 人类会逐渐接受事实，但是事实来得太过突然，情绪会崩溃。现代医学能够成功的使人类活的更长久、身体更健康、工作更多产，但是死亡不可避免，并且现代医学的成功，并没有提高对死亡的认知也没有提供给面临死亡的人足够的帮助。现代医学的成功之处在于将濒死的人类拉回阳光普照的人世间，但是这也增加了濒死之人面对死亡的时间！ 那么生命最后的愿望，每个人都会有，但是每个人都会实现吗？ 或许，站在生命的最后阶段，面对死神，舒服地离开最开心！","categories":[{"name":"书籍","slug":"书籍","permalink":"https://solaim.github.io/categories/%E4%B9%A6%E7%B1%8D/"}],"tags":[{"name":"life","slug":"life","permalink":"https://solaim.github.io/tags/life/"}]},{"title":"RabbitMQ安装和基本概念","slug":"RabbitMQ安装和基本概念","date":"2019-04-11T12:31:50.000Z","updated":"2023-09-10T13:21:16.209Z","comments":true,"path":"2019/04/11/RabbitMQ安装和基本概念/","link":"","permalink":"https://solaim.github.io/2019/04/11/RabbitMQ%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"RabbitMQ1、RabbitMQ消息队列RabbitMQ是一个消息队列：用于接收和转发消息。 2、RabbitMQ安装1| Mac安装brew install rabbitmq rabbitmq-server 2|Centos安装 安装epel源-解决erlang依赖wxGTK wget http://www.rpmfind.net/linux/centos/7.4.1708/extras/x86_64/Packages/epel-release-7-9.noarch.rpm yum install epel-release-7-9.noarch.rpm 添加erlang仓库-支持最新版erlang wget https://packages.erlang-solutions.com/erlang-solutions-1.0-1.noarch.rpm rpm -Uvh erlang-solutions-1.0-1.noarch.rpm 安装erlang # yum install erlang # 精简版 # yum install esl-erlang # 完整版 # 实际使用yum连网安装RabbitMQ时，会自动下载安装依赖，此步骤不进行，否则后期会引起冲突 安装RabbitMQ rpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.asc yum install rabbitmq-server-3.7.2-1.el7.noarch.rpm 启动Rabbit MQ systemctl enable rabbitmq-server systemctl start rabbitmq-server 3|ubuntu安装 添加erlang源 wget https://packages.erlang-solutions.com/erlang-solutions_1.0_all.deb dpkg -i erlang-solutions_1.0_all.deb 安装erlang # apt install esl-erlang # 不进行实际安装 安装rabbitmq echo &quot;deb https://dl.bintray.com/rabbitmq/debian xenial main&quot; | sudo tee /etc/apt/sources.list.d/bintray.rabbitmq.list wget -O- https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.asc | sudo apt-key add - apt update apt install rabbitmq-server 3、RabbitMQ使用http://blog.csdn.net/column/details/rabbitmq.html 问题：消息队列解决编程中的什么问题？或者不使用消息队列，可能存在哪些问题？ 这个问题的答案比较复杂。这取决于软件系统的复杂性。如果软件系统是由单独的一个模块构成，那么引入消息队列毫无用处。只是为了复杂而复杂。消息队列真正发挥作用的地方在于多模块或多组件或多子系统之间的交互以及异步任务。模块之间的通信，可以直接调用API。但是这种调用在单机系统可用，但是模块之间的耦合性太强。多机器之间的通信可以使用socket通信，但是需要维护连接，编程难度较大。 总结起来，存在的问题大概有： message的生产者和消费者如何维持连接，如果某方的连接中断，期间丢失的数据丢失了怎么办？ 如何降低生产者和消费者之间的耦合度？ 如何让优先级较高的消费者优先获取message？ 如何做到负载均衡？ 如何高效地将数据发送到相关的消费者？也就是说消费者如何接收不同类型的数据？ 如何进行扩展，甚至将消费者集群化？ 如何保证消费者接收了完整、正确的数据？ AMQP协议定义了解决这些问题的方案，RabbitMQ是一种具体实现。 生产者指任何产生数据的程序。 消费者指任何接收数据的程序。 queue:A queue is only bound by the host’s memory &amp; disk limits, it’s essentially a large message buffer. 消息队列涉及的概念： connection：建立client和server之间的连接 channel：建立在connection之上的逻辑连接 exchange：路由message,An exchange is a very simple thing. On one side it receives messages from producers and the other side it pushes them to queues. queue：存储和分发message bind：用于exchange和queue之间的绑定，可以理解为queue对某些message感兴趣 RabbitMQ包含两大部分：server和client。server充当broker，使用erlang实现，client是代码实现的重点，提供各种语言API。 RabbitMQ的核心理念是消息生产者不会直接将数据发送到队列。实际上，生产者只可以将数据发送到exchange。exchange主要做两件事：先从生产者接收数据，然后将数据推送到队列。exchange必须准确了解对接收到的数据进行什么操作。包括：是将数据推送到特定的队列，还是将数据推送到许多队列，或者干脆丢弃？这种操作取决于exchange的类型。RabbitMQ支持四种exchange：direct、topic、headers和fanout。 direct： topic： headers： fanout：将任何message广播到exchange知道的队列 exchange和队列创建完成之后，exchange需要知道将数据发送到哪个队列，这里需要进行exchange和队列之间的绑定。 开启rabbitmq管理网页的方法： rabbitmqctl start_app rabbitmq-plugins enable rabbitmq_management rabbitmqctl stop rabbitmq添加用户现添加admin，密码admin，管理员权限 rabbitmqctl add_user admin admin rabbitmqctl list_users rabbitmqctl set_user_tags admin administrator rabbitmqctl list_users 添加成功后赋权限 rabbitmqctl set_permissions -p / myuser &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;","categories":[],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"https://solaim.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"ansible","slug":"ansible","date":"2019-04-10T11:55:58.000Z","updated":"2023-09-10T13:21:16.225Z","comments":true,"path":"2019/04/10/ansible/","link":"","permalink":"https://solaim.github.io/2019/04/10/ansible/","excerpt":"","text":"1、 What is ansible? Ansible is a radically simple IT automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs. Ansible是一个颠覆性的运维工具，支持云配置、配置管理、应用部署、服务编配等运维工作。 Designed for multi-tier deployments since day one, Ansible models your IT infrastructure by describing how all of your systems inter-relate, rather than just managing one system at a time. Ansible一开始就是为多层架构设计的，因此Ansible通过描述系统内部关系，进行整体建模。 It uses no agents and no additional custom security infrastructure, so it’s easy to deploy - and most importantly, it uses a very simple language (YAML, in the form of Ansible Playbooks) that allow you to describe your automation jobs in a way that approaches plain English. Ansible no agents no additional custom security infrastucture, it use ssh describe automation jobs by yaml 2、How to Ansible?尽管各种平台的管理工具都可以安装Ansible，但是Ansible是Python开发的，所以Ansible的安装建议尽量使用pip: pip install ansible 如果想使用平台管理工具安装，具体查询官网。 检查安装结果： ansible --version 输出结果： ansible 2.7.6 config file = None configured module search path = [u&#39;/root/.ansible/plugins/modules&#39;, u&#39;/usr/share/ansible/plugins/modules&#39;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Nov 6 2016, 00:28:07) [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] 使用pip安装的Ansible没有自动生成配置文件，需要手动生成。使用平台工具安装会自动生成Ansible配置文件。 Ansible的核心是Playbooks。Playbooks是一个yaml格式的文本文件，用于描述事务状态。Playbooks的使用变量Variables描述，Variables可以在Playbooks、File、Inventories、Command line、Discovered variables、Ansible Tower等中定义。 Inventories类似中药药方，中药药方基本是这么写的： 当归莲子汤 当归 两钱 莲子 四钱 ... Inventories描述服务器的Host列表、范围、动态范围以及自定义。 Playbooks contain plays Plays contain tasks Tasks call modules Tasks run sequentially Handlers are triggered by Tasks, and are run once, at the end of Plays 高级Playbooks支持条件触发以及role等特征。定义好Playbooks之后，具体怎么运行Ansible呢？ 运行Ansible有三种主流的方式： Ad-Hoc：ansible -m Playbooks：ansible-playbook Automation Framework：Ansible Tower(AWX) 3、Could you give me a demo? 创建ansible目录 mkdir -p /etc/ansible 创建ansible.cfg touch /etc/ansible/ansible.cfg 编辑ansible.cfg # config file for ansible -- https://ansible.com/ # =============================================== # nearly all parameters can be overridden in ansible-playbook # or with command line flags. ansible will read ANSIBLE_CONFIG, # ansible.cfg in the current working directory, .ansible.cfg in # the home directory or /etc/ansible/ansible.cfg, whichever it # finds first [defaults] # some basic default values... inventory = /etc/ansible/hosts library = /usr/share/ansible/ #module_utils = /usr/share/my_module_utils/ #remote_tmp = ~/.ansible/tmp #local_tmp = ~/.ansible/tmp #plugin_filters_cfg = /etc/ansible/plugin_filters.yml forks = 5 #poll_interval = 15 sudo_user = root #ask_sudo_pass = True #ask_pass = True #transport = smart remote_port = 22 #module_lang = C #module_set_locale = False [inventory] [privilege_escalation] [paramiko_connection] [ssh_connection] [persistent_connection] [accelerate] [selinux] [colors] [diff] 详细配置文件，请参考Github网页。 编辑Inventory &#x2F;etc&#x2F;ansible&#x2F;hosts 192.168.0.142 使用Ad-hoc方式运行命令： ansible all -m ping ansible all -a &quot;/bin/echo hello&quot; 4、Yelp, but how to do with complex deployment?Playbooks is really good!!! Playbooks is really good!!! Playbooks is really good!!! 病比较复杂，就需要一系列工具来辅助治疗。最常用的工具有ansible和ansible-playbook。但是还有很多其他工具，比如： ansible-config: 用于查看、编辑、和管理ansible配置。 ansible-console：用于执行ansible ad-hoc 的repl console。 ansible-doc：插件文档工具，用于查看ansible modules的描述信息。 ansible-galaxy：ansible执行角色相关的操作。 ansible-inventory：用于查看inventory。 ansible-pull：从VCS仓库拉取playbooks并在本地执行。 ansible-vault：用于ansible数据文件加解密。 工具用的熟就好，但是主要工具不仅要用的熟，还得用的巧。ansible和ansible-playbook就是这样的工具。 ansible：用于定义和执行单一任务 ansible-playbook：运行ansible playbook，在目标主机群上执行定义的任务。 来看两个简单的ansible使用示例： ansible all -a &quot;/sbin/reboot&quot; -f 10 ansible all -m shell -a &#39;echo $TERM&#39; 第一个例子开启10个进程，让所有主机重启。当然，如果all中定义的主机数目小于10，比如5，那么有5个进程就是空闲的。如果all中有12个主机，那么有两个任务会等待。参数 -f 10 用来指定进程数目。 第二个例子打印TERM参数，但是使用-m指定了模块，一般默认的模块是’command’。 这两个都是简单的例子，ansible的模块支持更加复杂的任务。比如文件操作： ansible all -m copy -a &quot;src=/etc/hosts dest=/tmp/hosts&quot; # 类似于scp ansible all -m file -a &quot;dest=/srv/foo/a.txt mode=600&quot; ansible all -m file -a &quot;dest=/srv/foo/b.txt mode=600 owner=mdehaan group=mdehaan&quot; # 类似于chmod等命令 ansible webservers -m file -a &quot;dest=/path/to/c mode=755 owner=mdehaan group=mdehaan state=directory&quot; # 类似于mkdir -p ansible webservers -m file -a &quot;dest=/path/to/c state=absent&quot; # 类似于rm 比如管理包： ansible webservers -m yum -a &quot;name=acme state=present&quot; # 确保安装，但是不升级 ansible webservers -m yum -a &quot;name=acme-1.5 state=present&quot; # 确保安装指定版本 ansible webservers -m yum -a &quot;name=acme state=latest&quot; # 确保安装最新版本 ansible webservers -m yum -a &quot;name=acme state=absent&quot; # 确保未安装 比如用户和组的管理： ansible all -m user -a &quot;name=foo password=&lt;crypted password here&gt;&quot; # 创建用户 ansible all -m user -a &quot;name=foo state=absent&quot; # 删除用户 比如源码部署： ansible webservers -m git -a &quot;repo=https://foo.example.org/repo.git dest=/srv/myapp version=HEAD&quot; 比如服务管理： ansible webservers -m service -a &quot;name=httpd state=started&quot; # 开启 ansible webservers -m service -a &quot;name=httpd state=restarted&quot; # 重启 ansible webservers -m service -a &quot;name=httpd state=stopped&quot; # 关闭 比如限时后台任务： ansible all -B 3600 -P 0 -a &quot;/usr/bin/long_running_operation --do-stuff&quot; # /usr/bin/long_running_operation一个后台任务，-B表示超时时间，-P表示轮询polling ansible web1.example.com -m async_status -a &quot;jid=488359678239.2844&quot; # 使用async_status检查异步任务状态 ansible all -B 1800 -P 60 -a &quot;/usr/bin/long_running_operation --do-stuff&quot; # 在启动任务的同时开启轮询模式，任务执行30分钟，每60秒检查一次状态 比如获取系统信息： ansible all -m setup 嗯，ansible ad-hoc是很好用，但是我想把任务记录下来，下次直接执行，怎么办呢？ 这个就要用到ansible-playbook了，不过我们先来看看inventory。 ansible运行在复杂环境架构中，为了任务管理、运行方便，可以从inventory list中选取需要操作的主机组进行操作。inventory默认的存储文件是/etc/ansible/hosts，不过可以使用-i path进行指定。 inventory支持多文件同时配置，支持动态拉取，支持不同格式的文件。 mail.example.com [webservers] foo.example.com bar.example.com [dbservers] one.example.com two.example.com three.example.com 这是一个ini格式的inventory，[webservers]表示group，可以方便的选取执行相关任务的主机，并且表明主机的用途。yaml格式的像这样： all: hosts: mail.example.com: children: webservers: hosts: foo.example.com: bar.example.com: dbservers: hosts: one.example.com: two.example.com: three.example.com: 如果ssh使用的不是标准端口，需要在inventory中标明： badwolf.example.com:5309 inventory支持使用变量定义主机： jumper ansible_port=5555 ansible_host=192.0.2.50 ... hosts: jumper: ansible_port: 5555 ansible_host: 192.0.2.50 inventory支持模式匹配，如果主机类型模式类似，可以像这样： [webservers] www[01:50].example.com [databases] db-[a:f].example.com inventory支持定义连接类型和用户： [targets] localhost ansible_connection=local other1.example.com ansible_connection=ssh ansible_user=mpdehaan other2.example.com ansible_connection=ssh ansible_user=mdehaan 可以在inventory中直接定义变量，但是更常用的方法是在host_vars目录的单独文件中进行定义，然后在inventory中进行引用： [atlanta] host1 http_port=80 maxRequestsPerChild=808 host2 http_port=303 maxRequestsPerChild=909 [atlanta] host1 host2 [atlanta:vars] ntp_server=ntp.atlanta.example.com proxy=proxy.atlanta.example.com [atlanta] host1 host2 [atlanta:vars] ntp_server=ntp.atlanta.example.com proxy=proxy.atlanta.example.com host和group均支持变量。但是切记，这只是一种表示方法，实际执行的时候会用literal替代。 inventory有两个默认组：all和ungrouped。all包含所有主机，ungrouped包含没有分组的主机。当然inventory支持更复杂的定义方式，但是，这只是入门级使用手册，不一一赘述。快点进入重点吧，怎么用Playbooks呢？ 如果你有一间作坊，那么modules就是你的工具，Playbooks就是你的操作手册，inventory就是原料。 记住，Playbooks是ansible的语言、的语言、的语言… 5、Oh，No！sophisticated Playbooks…- hosts: webservers remote_user: root tasks: - name: ensure apache is at the latest version yum: name: httpd state: latest - name: write the apache config file template: src: /srv/httpd.j2 dest: /etc/httpd.conf - hosts: databases remote_user: root tasks: - name: ensure postgresql is at the latest version yum: name: postgresql state: latest - name: ensure that postgresql is started service: name: postgresql state: started 这个Playbooks示例中，有两个play，第一个现在webservers group执行apache安装和配置，第二个在databases group执行postgresql安装和启动。Playbooks顺序执行。 Playbooks contain plays Plays contain tasks Tasks call modules Tasks run sequentially Handlers are triggered by Tasks, and are run once, at the end of Plays 一个play必须指定目标主机和操作用户，注意是远程操作用户。目标主机通过-hosts指定，操作用户使用remote_user指定，支持细分task指定不同操作用户，支持用户切换。hosts是目标主机的列表，若有多组目标主机，使用冒号分割。一个play可以有多个task，每个task必须指定任务名称，任务调用模块，模块的调用格式为module: options。options分为key-value形式和非key-value形式的。 tasks: - name: make sure apache is running service: name: httpd state: started # key-value tasks: - name: enable selinux command: /sbin/setenforce 1 # non key-value 根据上面的两个例子，可以看出options的格式根据modules发生变化。任务可以出发handler，比如两个任务task均涉及到nginx配置文件的修改，在task 的notify中均定义了handler，那么在play执行的最后，将只执行一次handler，避免nginx重复重启。 name: template configuration file template: src: template.j2 dest: /etc/foo.conf notify: - restart memcached - restart apache handlers: - name: restart memcached service: name: memcached state: restarted - name: restart apache service: name: apache state: restarted 执行Playbooks的命令行： ansible-playbook playbook.yml 一个简单的nginx安装示例： - hosts: all remote_user: root tasks: - name: yum install nginx yum: name: nginx state: latest","categories":[{"name":"Python","slug":"Python","permalink":"https://solaim.github.io/categories/Python/"}],"tags":[{"name":"Python, Ansible","slug":"Python-Ansible","permalink":"https://solaim.github.io/tags/Python-Ansible/"}]},{"title":"人工智能","slug":"人工智能","date":"2017-12-19T12:44:16.000Z","updated":"2023-09-10T13:21:16.190Z","comments":true,"path":"2017/12/19/人工智能/","link":"","permalink":"https://solaim.github.io/2017/12/19/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","excerpt":"","text":"本文是李开复先生《人工智能》一书的摘录，结合自己的感想而成。若对人工智能具有兴趣，请移步李开复先生的书籍，或者人工智能专业书籍。 人工智能就在我们生活的每一处人工智能来了！人工智能来了！人工智能来了！重要的事情必须说三遍！ 恐惧不？害怕失业不？有没有可能下一秒被人工智能干掉？ 是不是各种担心充满心头？ 扯一句韩国电视剧《一起用餐吧》中的台词，“担心又不会解决所有问题，还不如开心的活着。”没有去翻具体的台词，大意如此。 人工智能开始真实的进入人类的生活，对于各个层级的人，具有不同的意义。但是什么是人工智能？ 人工智能必须长得像人吗？答案来的很简单，不是。 我们做好与人工智能共同发展的准备了吗？答案不是很明晰。 我们做好接受人工智能产品与我们一起生活、工作了吗？答案是不知道。 感觉不靠谱？ 人类对出现的新事物具有本能的好奇心和探索欲，以及多多少少的恐惧。 赶快打开你的手机，看看你的手机看看有多少个app使用了人工智能技术？让我猜猜，0个，3个，7个… 滴滴出行、谷歌翻译、手机淘宝、今日头条、美图秀秀等等。什么，看着不像啊？不就是个app么？ “很多时候人工智能都是藏在底下，因此人们并不知道有很多东西已经是机器学习的系统在驱动…“，谷歌大脑开发团队带头人杰夫·迪恩如是说。 那么到底什么是人工智能？该如何定义？作者介绍了五种定义，让我们来看看： 定义一：AI就是让人觉得不可思议的计算机程序 定义二：AI就是与人类思考方式相似的计算机程序 定义三：AI就是与人类行为相似的计算机程序 定义四：AI就是会学习的计算机程序 定义五：AI就是根据对环境的感知，做出合理的行动，并获得最大收益的计算机程序 有什么想法？ Nothing？管它该怎么定义，有这个时间，还不如鼓捣鼓捣程序。原谅程序猿的fan。 人工智能的三生三世三盘棋象征了人工智能的三生三世。没看过这个电视剧，只是借用一下。 第一盘棋是1962年，IBM的西洋跳棋程序战胜一位人类盲人跳棋高手。跳棋是什么，没玩过。 第二盘是1997年，IBM的深蓝战胜国际象棋高手卡斯帕罗夫。 第三盘是2016年，AlphaGo战胜围棋大师李世石。 三盘棋，象征着人工智能的三个时代。三个时代，又有什么不同？先来回答一下相同之处，每次人工智能胜利之后，人类会燃起看似无尽的热情，但是深入了解之后，由发出不过尔尔的叹慨。 高德纳的技术成熟度曲线倒是很有意思。几乎每项技术，在成熟之前，都会在起起伏伏中发展，当然有些被彻底抛弃了，幸存下来的，经过冰与火的洗礼，成为对人类有用的工具。是的，只是工具。是不是太绝对了？ 那么人类如何确定某事物是不是有用了？ 根据心理学，人们接受一件新事物，就像人们感受一种外界刺激一样，是有一个心理阀值的。只有强于这个阀值的刺激，才会被人类明确感受。 而人工智能的第三世，已经在机器视觉、语音识别、数据挖掘、自动驾驶等具体领域突破人类的心理阀值，并且在人类社会生活中发挥重要作用。在这一过程中，深度学习，居功甚伟。可以说，目前没有深度学习做不了的。深度学习这么厉害，那人工智能的发展又经历了那几个阶段呢？ 人工智能的第一生：图灵测试与人机对话 人工智能的第二生：语音识别 “计算机的思维方法与人类的思维方法之间，似乎存在着非常微妙的差异，以至于在计算机科学的实践中，越是抛弃人类既有的经验知识，依赖于问题本身的数据特征，越是容易得到更好的结果。” 人工智能的第三生：深度学习携手大数据 深度学习代表了第三代人工智能。深度学习是什么？为什么在人工智能的第三代独占鳌头？ 千古神兵，重出江湖，必定“腥风血雨”。除去宝剑的鞘，才能看见宝剑的锋利。深度学习说到底就是人工神经网络。可惜啊，三年前，与这大宝剑失之交臂，玩了三年的遗传算法和粒子群算法，不甚了了。 “有时对问题领域的扩展可以让问题变得更简单！”，马文·闵斯基说。 为什么深度学习在人工智能的第三代才开始发力？有三个原因，深度学习在1986年开始流行时，深度学习的理论无法解决网络层次加深带来的诸多问题，其次当时的计算能力远远达不到深度神经网络的需要，最重要的是深度学习大展神威的祭品-大规模海量数据没有准备好。 人机PK，奇点是否真的来临人机PK，AlphaGo赢了。又怎样？ 更多的是一种警示：“如果计算机可以在2年内实现大多数人此前预测要花20年或更长时间才能完成的进步，那么，还有哪些突破会以远超常人预期的速度来临？这些突破会不会超出我们对人工智能的想象，颠覆人类预想中的未来？我们已为这些即将到来的技术突破做好准备了吗？” AlphaGo给了人类一个机会，重新思考人工智能的机会。人工智能是什么？人工智能之于人类的意义？人工智能会怎样影响人类的生活与工作？人工智能会不会在未来将人类挑落马下？ 先不论AI与人类的PK，AI与AI之间的竞赛，会不断促进AI提高。人类虽望尘莫及，但可以不断从AI中学习新的思想。其实，在现阶段，担忧AI将人类挑落马下，有些操之过急。为何如此说？因为AI现阶段处于弱人工智能、强人工智能和超人工智能的三部曲的弱人工智能，专注于解决特定领域的问题。而强人工智能将能胜任人类所有的工作，发展到超人工智能，人工智能的智能将远胜人类。谁知道，超人工智能是个什么鬼样子？ 奇点是否来临？ 谁知道了，反正很多事，我都是后知后觉。 反正AI在跨领域推理、抽象能力、知其然知其所以然、常识、自我意识、审美和情感等方面还很稚嫩。 但是提早规划又有什么错呢？ AI时代的变革每一项重大的技术变革，都将引起社会生活的改变，或大或小。那么人工智能给人类的社会生活带来的是什么呢？ 牛津大学的伊安·戈尔丁认为世界面临三个挑战： 1、人类赶不上科技发展的速度，来不及调整适应； 2、人脉之间的相互连接以及信息的迅速传播，即有好的一面，也有危险的一面； 3、对个人和国家短期有益的事情，有可能伤及世界的整体利益。 “技术不仅仅是技术。技术的未来必将与社会的未来、经济的未来、文学艺术的未来、人类全球化的未来紧密联系在一起。” 单独谈论人工智能的未来，没有实际意义。“人工智能的未来必将与重大的社会经济变革、教育变革、思想变革、文化变革等同步。” 人工智能快速发展迎来的第一个诘问是：“人工智能会不会引起大面积失业”？ 肯定会的。由于人工智能的快速发展，一些简单重复性的、不具备创造性的工作将会被替代。但是工作会消失吗？ 肯定不会。新的产业将会替代旧的产业。所以，作为一名职场工作者，必须学会分析那些工作不会被轻易替代，考虑如何利用人工智能技术提高自己的战斗力，在人机协作的新工作氛围中找到自己的位置。 ”AI只是人类的工具。技术本身不是问题，问题是我们如何使用技术以及如何围绕人工智能这样一种革命性的新科技，建立与之配合的社会和经济结构，用制度来保证人人都可享有人工智能大的巨大收益，同时不必担心失业等潜在风险。“ 变革毫无疑问，问题是人类将会迎来什么样的变革。过去到现在，人类一直在寻找金字塔结构的平衡，不管是皇权时代还是民主时代。但是人工智能时代到来之后，简单可重复、毫无创造性的工作将会被大面积取代，这会不会引起社会结构的变革？ ”AI的未来掌握在哪些创造、开发和使用者的手中。无疑地，AI会改变世界，但这里真正的问题是，改变AI的又是谁呢？“李飞飞抛出的这个问题又该如何解答？ 或许更好的答案就是作者提到的：增加人工智能研究者的多样性，社会结构螺旋上升到一个新的平衡状态。 AI先行AI先行，是时代的选择，就像每个时代都有每个时代的标志，每个时代都将留下自己的烙印。AI重在提升效率，而非发明新的流程、新的业务。 AI时代，最大的风口就是人工智能本身。 肯定不是所有猪都能在风口上飞起来，但要做一飞冲天的创业英雄，就一定要看准科技大势，选择最正确的时机做最正确的事。 但是当猪都飞起来之后，猪与猪的相互摩擦会不会引起一片火海，将猪都做成烤猪呢？ AI时代AI时代，必将与往不同。那么，个人如何迎接AI时代的冲击与挑战呢？我们势必需要从三个方面来回答。首先是怎么学的问题，其次是学什么的问题，最后最难回答的问题：有了AI，人生还有意义吗？ 李开复先生建议的AI时代的学习方法包括： 主动挑战极限 从实践中学习 关注启发式教育，培养创造力和独立解决问题的能力 在线学习 主动向机器学习 既学习人-人合作，也学习人-机写作 学习要追随兴趣 一一列举AI时代该学什么是很难的，但是李开复先生给出了一个基本思路： “人工智能时代，程式化的、重复性的、仅靠记忆与练习就可以掌握的技能将是最没有价值的技能，几乎一定可以由机器来完成；反之，那些最能体现人的综合素质的技能，例如，人对于复杂系统的综合分析、决策能力，对于艺术和文化的审美能力和创造性思维，有生活经验及文化熏陶产生的直觉、常识，基于人自身的情感与他人互动的能力…这就是人工智能时代最有价值，最值得培养、学习的技能。而且这些技能中，大多数都是因人而异，需要“定制化”教育和培养，不可能从传统的批量教育中获取。” 浮生碌碌，汲汲营营，我们身为万物之灵，到底该怎样活着？ 人生在世，无论是理性还是感性，我们所能知、能见、能感的实在太有限了。 人只不过是一根苇草，但却是一根能思想的苇草。","categories":[{"name":"书籍","slug":"书籍","permalink":"https://solaim.github.io/categories/%E4%B9%A6%E7%B1%8D/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://solaim.github.io/tags/AI/"}]}],"categories":[{"name":"k8s","slug":"k8s","permalink":"https://solaim.github.io/categories/k8s/"},{"name":"Golang","slug":"Golang","permalink":"https://solaim.github.io/categories/Golang/"},{"name":"Python","slug":"Python","permalink":"https://solaim.github.io/categories/Python/"},{"name":"书籍","slug":"书籍","permalink":"https://solaim.github.io/categories/%E4%B9%A6%E7%B1%8D/"}],"tags":[{"name":"Golang, Kubernetes, Service","slug":"Golang-Kubernetes-Service","permalink":"https://solaim.github.io/tags/Golang-Kubernetes-Service/"},{"name":"Golang, Prometheus, Observability","slug":"Golang-Prometheus-Observability","permalink":"https://solaim.github.io/tags/Golang-Prometheus-Observability/"},{"name":"Python, Flask, SocketIO","slug":"Python-Flask-SocketIO","permalink":"https://solaim.github.io/tags/Python-Flask-SocketIO/"},{"name":"life","slug":"life","permalink":"https://solaim.github.io/tags/life/"},{"name":"消息队列","slug":"消息队列","permalink":"https://solaim.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Python, Ansible","slug":"Python-Ansible","permalink":"https://solaim.github.io/tags/Python-Ansible/"},{"name":"AI","slug":"AI","permalink":"https://solaim.github.io/tags/AI/"}]}