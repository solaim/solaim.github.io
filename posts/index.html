<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Jerry Wang</title>
<meta name=keywords content><meta name=description content="Posts - Jerry Wang"><meta name=author content="Jerry Wang"><link rel=canonical href=https://solaim.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://solaim.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://solaim.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://solaim.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://solaim.github.io/apple-touch-icon.png><link rel=mask-icon href=https://solaim.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://solaim.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://solaim.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://solaim.github.io/posts/"><meta property="og:site_name" content="Jerry Wang"><meta property="og:title" content="Posts"><meta property="og:description" content="Jerry Wang's Personal Blog"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta property="og:image" content="https://solaim.github.io/img/favicon.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://solaim.github.io/img/favicon.webp"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Jerry Wang's Personal Blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://solaim.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://solaim.github.io/ accesskey=h title="首页 (Alt + H)"><img src=https://solaim.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://solaim.github.io/categories/ title=🕸分类><span>🕸分类</span></a></li><li><a href=https://solaim.github.io/tags/ title=🐚标签><span>🐚标签</span></a></li><li><a href=https://solaim.github.io/about/ title=🌏关于><span>🌏关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://solaim.github.io/>Home</a></div><h1>Posts
<a href=/posts/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>AI Agent纵览</h2></header><div class=entry-content><p>AI Agent最近两三年异常火热，下面👇是某头部公司的AI Agent开发岗的JD：
岗位职责
1、负责大模型应用开发范式及一站式研发平台的设计、开发及应用，支持公司内部大模型的应用的快速落地
2、负责大模型产品的整体技术解决方案，并支持技术研发和效果优化等工作，推动产品持续增长
3、保障大模型应用系统架构的稳定、高效运行，帮助业务优化性能和改善系统稳定性
职位描述
1、计算机或相关专业本科及以上学历，2年以上工作经验，熟练掌握Python，或至少一门主流开发语言。
2、有Agent平台/应用开发经验者优先，熟悉coze，dify，langflow等开发平台，了解RAG、Agent机制，使用过langchain等开源系统，有prompt工程业务优化经验者优先。
3、有比较敏锐的产品意识和数据分析能力。
4、有优秀的架构设计能力，了解包括检索引擎、kv存储、在线应用框架、批流处理等中间件系统，有承担过千万级以上产品应用架构设计者优先。
从上面的JD可以提取的信息：
开发语言：Python(C++、Golang需求也较多) 框架：dify、langflow等 原理：RAG、Agent工作原理 后端基础组件 AI Agent AI Agent（也称人工智能代理、智能体）是一种能够感知环境、进行决策和执行动作的智能实体。智能体像人一样，它有记忆、有逻辑分析能力、有任务的拆解能力、问题的拆解能力和最后综合回来统一解决问题的能力。
智能体的组成：
I：输入，也就是上图中的感知Perception，包括文本、图片、视频、音频等 O：输出，即上图中的行动Action，包括输出文本及调用工具等 B：大脑Brain，基于LLM，具有知识、记忆、推理、规划等功能 智能体的形式：
单Agent：单个Agent完成任务 多Agent：多个Agent配合完成任务 RAG 检索增强生成（Retrieval Augmented Generation），简称 RAG。
为什么会有RAG？
解决知识缺失问题：大模型的知识固定在训练的数据上，无法获取最新资讯。 解决快问乱答问题：大模型有幻觉问题，超出范围的答案不靠谱。 解决数据安全问题：企业专有资料，不能丢给通用大模型去训练。 实现路径：
**
将知识向量化：知识通过embedding存储在向量数据库 客户检索：问题embedding，检索向量数据库，生成提示词 投喂LLM：将上面生成的提示词喂给LLM生成最终的答案 MCP Model Context Protocol（模型上下文协议）是 Anthropic 在推出的用于 LLM 应用和外部数据源（Resources）或工具（Tools）通信的标准协议，遵循 JSON-RPC 2.0 的基础消息格式。
MCP遵循CS架构，应用包括client，client负责和server通信。
核心概念：
资源：MCP 服务器希望提供给客户端的任何类型的数据 提示词：MCP 服务器能够定义可重用的提示模板和工作流 工具：使服务器能够向客户端公开可执行功能。通过工具，LLM 可以与外部系统交互，执行计算，并在真实的世界中采取行动。 采样：服务器通过客户端请求 LLM 根：定义了服务器可以操作的边界 详细文档请参考官方文档
...</p></div><footer class=entry-footer><span title='2025-05-07 10:29:29 +0800 +0800'>2025-05-07</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to AI Agent纵览" href=https://solaim.github.io/posts/ai_agent%E7%BA%B5%E8%A7%88/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>ROS2架构和实现</h2></header><div class=entry-content><p>ROS2是机器人领域的著名SDK，可以加速学习效率。ROS2主要解决了两个问题：
通信：基于Node，包含topic、service、action通信方式 代码架构：模块化开发，简化维护 一、架构 userland: code
client: rclc rclcpp rclpy etc…
abstract: rcl
middleware: rmw rmw_implementation rmw_fastrtps etc …
dds: fastdds Context_RTI etc…
二、实现 ROS2代码惯例：
./ ../ include/... # 头文件 src/... # 实现 1、Node实现 推荐继承Node实现业务功能, 主要选择合理的通信方式，注册回调函数。
namespace palomino { class VincentDriver : public rclcpp::Node { // ... }; } int main(int argc, char * argv[]) { rclcpp::init(argc, argv); rclcpp::spin(std::make_shared&lt;palomino::VincentDriver>()); rclcpp::shutdown(); return 0; } Node的声明：
// node.hpp class Node : public std::enable_shared_from_this&lt;Node> { public: explicit Node( const std::string & node_name, const NodeOptions & options = NodeOptions()); explicit Node( const std::string & node_name, const std::string & namespace_, const NodeOptions & options = NodeOptions()); virtual ~Node(); // 创建发布者 template&lt; typename MessageT, typename AllocatorT = std::allocator&lt;void>, typename PublisherT = rclcpp::Publisher&lt;MessageT, AllocatorT>> std::shared_ptr&lt;PublisherT> create_publisher( const std::string & topic_name, const rclcpp::QoS & qos, const PublisherOptionsWithAllocator&lt;AllocatorT> & options = PublisherOptionsWithAllocator&lt;AllocatorT>() ); // 创建订阅者 template&lt; typename MessageT, typename CallbackT, typename AllocatorT = std::allocator&lt;void>, typename SubscriptionT = rclcpp::Subscription&lt;MessageT, AllocatorT>, typename MessageMemoryStrategyT = typename SubscriptionT::MessageMemoryStrategyType > std::shared_ptr&lt;SubscriptionT> create_subscription( const std::string & topic_name, const rclcpp::QoS & qos, CallbackT && callback, const SubscriptionOptionsWithAllocator&lt;AllocatorT> & options = SubscriptionOptionsWithAllocator&lt;AllocatorT>(), typename MessageMemoryStrategyT::SharedPtr msg_mem_strat = ( MessageMemoryStrategyT::create_default() ) ); // 创建service的客户端 template&lt;typename ServiceT> typename rclcpp::Client&lt;ServiceT>::SharedPtr create_client( const std::string & service_name, const rclcpp::QoS & qos = rclcpp::ServicesQoS(), rclcpp::CallbackGroup::SharedPtr group = nullptr); // 创建service的服务端 template&lt;typename ServiceT, typename CallbackT> typename rclcpp::Service&lt;ServiceT>::SharedPtr create_service( const std::string & service_name, CallbackT && callback, const rclcpp::QoS & qos = rclcpp::ServicesQoS(), rclcpp::CallbackGroup::SharedPtr group = nullptr); // 参数相关操作方法 // 其他服务方法 protected: Node( const Node & other, const std::string & sub_namespace); private: rclcpp::node_interfaces::NodeBaseInterface::SharedPtr node_base_; rclcpp::node_interfaces::NodeGraphInterface::SharedPtr node_graph_; rclcpp::node_interfaces::NodeLoggingInterface::SharedPtr node_logging_; rclcpp::node_interfaces::NodeTimersInterface::SharedPtr node_timers_; rclcpp::node_interfaces::NodeTopicsInterface::SharedPtr node_topics_; rclcpp::node_interfaces::NodeServicesInterface::SharedPtr node_services_; rclcpp::node_interfaces::NodeClockInterface::SharedPtr node_clock_; rclcpp::node_interfaces::NodeParametersInterface::SharedPtr node_parameters_; rclcpp::node_interfaces::NodeTimeSourceInterface::SharedPtr node_time_source_; rclcpp::node_interfaces::NodeTypeDescriptionsInterface::SharedPtr node_type_descriptions_; rclcpp::node_interfaces::NodeWaitablesInterface::SharedPtr node_waitables_; const rclcpp::NodeOptions node_options_; const std::string sub_namespace_; const std::string effective_namespace_; class NodeImpl; // This member is meant to be a place to backport features into stable distributions, // and new features targeting Rolling should not use this. // See the comment in node.cpp for more information. std::shared_ptr&lt;NodeImpl> hidden_impl_{nullptr}; }; 2、启动 上面的代码示例中，main函数调用了三个ROS2 C++客户端库的函数：
...</p></div><footer class=entry-footer><span title='2025-05-07 10:20:33 +0800 +0800'>2025-05-07</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to ROS2架构和实现" href=https://solaim.github.io/posts/ros2%E6%9E%B6%E6%9E%84%E5%92%8C%E5%AE%9E%E7%8E%B0/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>说说k8s网络</h2></header><div class=entry-content><p>1、k8s组件 k8s的组件分为两大部分：控制平面组件和节点组件。
控制平面组件包括：
kube-apiserver：API server，可以水平扩展。 etcd：后端存储。 kube-scheduler：决策新创建的Pod在哪个Node上运行。 kube-controller-manager：controller进程，观测Object的状态，current state -> desired state。 节点组件包括：
kubelet：运行Pod。 kube-proxy：网络代理， k8s中Service的部分实现，负责维护节点上的网络规则。 Container runtime：Docker、containerd、CRI-O。 2、k8s网络需要解决的问题 k8s对任何CNI实现有如下规定：
Pod与Pod之间的通信无须使用NAT 节点与Pod之间的通信无须使用NAT Pod与其他Pod看到自身的IP相同 k8s中的Service负责对内、对外暴露网络访问，想让应用能够在集群内外正常访问，需要解决下面的几个问题：
容器与容器的通信 Pod与Pod的通信 Pod与Service的通信 Internet与Service的通信 3、容器与容器的通信 在Linux中，使用Namespace机制实现内核级别资源的隔离，目前提供六种Namespace：
Mount: 隔离文件系统挂载点 UTS: 隔离主机名和域名信息 IPC: 隔离进程间通信 PID: 隔离进程的ID Network: 隔离网络资源 User: 隔离用户和用户组的ID 网络命名空间netns存在一个root network namespace，默认情况下，Linux将每个进程的netns设置为root，以提供网络访问。
对于Docker，Pod使用同一个网络命名空间，Pod之间通过Namespace隔离，Pod内的容器共享网络命名空间，Docker负责创建网络命名空间，应用容器使用-network加入该网络命名空间，容器之间通过localhost通信。
4、Pod与Pod的通信 k8s中，每个Pod拥有一个真实的IP，Pod之间通过IP通信。
（本节图片来源：Kevin Sookocheff Blog）
5、Pod与Service的通信 Pod随着时间，可能消失、重启，那么直接使用Pod IP进行访问，在动态变化的环境中会存在网络问题。k8s中使用Service解决这个问题。（本节图片来源：Kevin Sookocheff Blog）
创建一个新的Service Object，实际上是创建了一个虚拟IP和一系列网络规则。
kube-proxy负责维护、更新、删除、添加网络规则，其支持的代理模式有：
userspace iptables（默认） ipvs kernelspace (windows) ...</p></div><footer class=entry-footer><span title='2021-06-06 11:17:45 +0800 +0800'>2021-06-06</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to 说说k8s网络" href=https://solaim.github.io/posts/%E8%AF%B4%E8%AF%B4k8s%E7%BD%91%E7%BB%9C/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Prometheus内部实现(四)</h2></header><div class=entry-content><p>目标该怎么管理？静态还是动态发现？
目标管理是使用静态还是动态发现，这个和基础架构有关，要回答这个问题，需要先说说应用部署的现状！
目前，企业应用部署有两种选项：
基于虚拟机 基于容器 当然，更多的是虚拟机里装docker，算是两者的结合，节省成本。
那么基于虚拟机部署应用有什么特点呢？就是变化小，你的预期和实际情况是相对静止的，在虚拟机上部署一套应用，一套流程走下来，大概得小半天吧，这种情况把要监控的目标写到Prometheus的配置文件，再重启Prometheus server，完全玩得转！
然而，当公司的应用开始基于容器部署，特别是管控平面使用了k8s，那么手动配置Prometheus、重启就成为了噩梦一样的存在！运维会疲于奔命，开发会吐槽运维。
所以监控目标的配置，取决于使用的底层技术！
题外话：要想Devops在企业落地，得有一整套配套的工具，整体打通。
那么Prometheus在抓取目标方面是怎么做的呢？我们先重温一下相关组件的启动：
{ // Scrape discovery manager. g.Add( func() error { err := discoveryManagerScrape.Run() level.Info(logger).Log("msg", "Scrape discovery manager stopped") return err }, func(err error) { level.Info(logger).Log("msg", "Stopping scrape discovery manager...") cancelScrape() }, ) } { // Notify discovery manager. ... } { // Scrape manager. g.Add( func() error { // When the scrape manager receives a new targets list // it needs to read a valid config for each job. // It depends on the config being in sync with the discovery manager so // we wait until the config is fully loaded. &lt;-reloadReady.C err := scrapeManager.Run(discoveryManagerScrape.SyncCh()) level.Info(logger).Log("msg", "Scrape manager stopped") return err }, func(err error) { // Scrape manager needs to be stopped before closing the local TSDB // so that it doesn't try to write samples to a closed storage. level.Info(logger).Log("msg", "Stopping scrape manager...") scrapeManager.Stop() }, ) } 启动的核心语句：
...</p></div><footer class=entry-footer><span title='2021-03-22 21:31:03 +0800 +0800'>2021-03-22</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to Prometheus内部实现(四)" href=https://solaim.github.io/posts/prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E5%9B%9B/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Prometheus内部实现(三)</h2></header><div class=entry-content><p>非线性 => 线性，混乱 => 秩序
上一篇博文主要写了Prometheus server的启动过程，但是并没有深入各个组件，了解其具体实现，这篇文件主要介绍Initial configuration loading部分。
Prometheus server的main函数有两个很精彩的实现，一个是reloader的注册，一个是组件启动。但是有个问题，所有的组件都是同时启动的吗？还是有什么顺序？
我们列一个组件清单：
- Termination handler. - Scrape discovery manager. - Notify discovery manager. - Scrape manager. - Reload handler. - Rule manager. - TSDB. - Web handler. - Notifier. 再来看一下组件启动使用的底层技术:
run.Group is a universal mechanism to manage goroutine lifecycles.
Create a zero-value run.Group, and then add actors to it. Actors are defined as a pair of functions: an execute function, which should run synchronously; and an interrupt function, which, when invoked, should cause the execute function to return. Finally, invoke Run, which concurrently runs all of the actors, waits until the first actor exits, invokes the interrupt functions, and finally returns control to the caller only once all actors have returned. This general-purpose API allows callers to model pretty much any runnable task, and achieve well-defined lifecycle semantics for the group.
...</p></div><footer class=entry-footer><span title='2021-03-20 21:29:33 +0800 +0800'>2021-03-20</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to Prometheus内部实现(三)" href=https://solaim.github.io/posts/prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E4%B8%89/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Prometheus内部实现(二)</h2></header><div class=entry-content><p>观察事物，需要提纲挈领！软件的启动是观察软件最好的入口！
Prometheus server实现入口文件在cmd/prometheus/main.go。
Prometheus的配置由两个部分构成：
配置文件：prometheus.yml 启动flag：启动时指定 入口函数如下：
func main() { ... } 初始化一个flagConfig，这个配置贯穿了整个程序
cfg := flagConfig{ notifier: notifier.Options{ Registerer: prometheus.DefaultRegisterer, }, web: web.Options{ Registerer: prometheus.DefaultRegisterer, Gatherer: prometheus.DefaultGatherer, }, promlogConfig: promlog.Config{}, } 使用kingpin注册flag
a := kingpin.New(filepath.Base(os.Args[0]), "The Prometheus monitoring server").UsageWriter(os.Stdout) a.Version(version.Print("prometheus")) a.HelpFlag.Short('h') a.Flag("config.file", "Prometheus configuration file path."). Default("prometheus.yml").StringVar(&amp;cfg.configFile) a.Flag("web.listen-address", "Address to listen on for UI, API, and telemetry."). Default("0.0.0.0:9090").StringVar(&amp;cfg.web.ListenAddress) ... _, err := a.Parse(os.Args[1:]) if err != nil { fmt.Fprintln(os.Stderr, errors.Wrapf(err, "Error parsing commandline arguments")) a.Usage(os.Args[1:]) os.Exit(2) } 加载配置文件，检查配置合法性
if _, err := config.LoadFile(cfg.configFile); err != nil { level.Error(logger).Log("msg", fmt.Sprintf("Error loading config (--config.file=%s)", cfg.configFile), "err", err) os.Exit(2) } 初始化各种组件
var ( localStorage = &amp;readyStorage{} scraper = &amp;readyScrapeManager{} remoteStorage = remote.NewStorage(log.With(logger, "component", "remote"), prometheus.DefaultRegisterer, localStorage.StartTime, cfg.localStoragePath, time.Duration(cfg.RemoteFlushDeadline), scraper) fanoutStorage = storage.NewFanout(logger, localStorage, remoteStorage) ) var ( ctxWeb, cancelWeb = context.WithCancel(context.Background()) ctxRule = context.Background() notifierManager = notifier.NewManager(&amp;cfg.notifier, log.With(logger, "component", "notifier")) ctxScrape, cancelScrape = context.WithCancel(context.Background()) discoveryManagerScrape = discovery.NewManager(ctxScrape, log.With(logger, "component", "discovery manager scrape"), discovery.Name("scrape")) ctxNotify, cancelNotify = context.WithCancel(context.Background()) discoveryManagerNotify = discovery.NewManager(ctxNotify, log.With(logger, "component", "discovery manager notify"), discovery.Name("notify")) scrapeManager = scrape.NewManager(log.With(logger, "component", "scrape manager"), fanoutStorage) opts = promql.EngineOpts{ Logger: log.With(logger, "component", "query engine"), Reg: prometheus.DefaultRegisterer, MaxSamples: cfg.queryMaxSamples, Timeout: time.Duration(cfg.queryTimeout), ActiveQueryTracker: promql.NewActiveQueryTracker(cfg.localStoragePath, cfg.queryConcurrency, log.With(logger, "component", "activeQueryTracker")), LookbackDelta: time.Duration(cfg.lookbackDelta), NoStepSubqueryIntervalFn: noStepSubqueryInterval.Get, EnableAtModifier: cfg.enablePromQLAtModifier, EnableNegativeOffset: cfg.enablePromQLNegativeOffset, } queryEngine = promql.NewEngine(opts) ruleManager = rules.NewManager(&amp;rules.ManagerOptions{ Appendable: fanoutStorage, Queryable: localStorage, QueryFunc: rules.EngineQueryFunc(queryEngine, fanoutStorage), NotifyFunc: sendAlerts(notifierManager, cfg.web.ExternalURL.String()), Context: ctxRule, ExternalURL: cfg.web.ExternalURL, Registerer: prometheus.DefaultRegisterer, Logger: log.With(logger, "component", "rule manager"), OutageTolerance: time.Duration(cfg.outageTolerance), ForGracePeriod: time.Duration(cfg.forGracePeriod), ResendDelay: time.Duration(cfg.resendDelay), }) ) scraper.Set(scrapeManager) 将配置更新到flagConfig
...</p></div><footer class=entry-footer><span title='2021-03-15 21:28:09 +0800 +0800'>2021-03-15</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to Prometheus内部实现(二)" href=https://solaim.github.io/posts/prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E4%BA%8C/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Prometheus内部实现(一)</h2></header><div class=entry-content><p>Prometheus在监控界为什么这么🔥？
Prometheus具体是怎么实现的？
人如其名，产品亦是！Prometheus就是火种，照耀在当前的监控世界！
Prometheus这把火，在我看来，火在三个方面：
系出名门：CNCF的第二个项目，CNCF的老大叫K8S，老大对老二很照顾，监控首选。 门庭若市：开源，大厂竞相争用。 简单易用：架构简单，配置简单，上下游打通。 大家都在用，而且用起来很方便的确是我们选择产品的方法，但是作为开发人员，还是需要了解一下其内部的实现。这个系列，主要根据Prometheus的代码和文档，梳理Prometheus的内部实现。
总体架构 Prometheus从配置的jobs中抓取metrics，存储在本地存储或者外部存储中，在metrics上运行规则聚合多维度的数据或者生成告警。Grafana或者自带的web UI能够方便的展示数据。
Prometheus的组件包括：
server：负责抓取metrics，存储metrics，分析、聚合metrics client library：多种语言的客户端库，方便实现 push gateway：短期存活服务metrics推送网关 exporters：比如监控节点的node_exporter，监控MySQL的mysqld_expoter等等 alertmanager：alertmanager负责告警的发送 从总体架构，我们可以了解各种组件，以及数据的流向，现在我们开始研究Prometheus server的内部实现。
Server实现架构 server启动的时候，根据配置文件prometheus.yml和flag决定各种配置，配置文件包括全局配置、抓取目标配置和规则配置等等。scrape_configs配置了抓取的目标，Prometheus支持动态获取目标，Service discovery读取静态配置的目标或者周期更新动态目标，添加到scrape manager，scrape manager负责抓取目标，抓取实际是一个HTTP请求，即各种exporters实际上就是一个简单的HTTP服务，负责暴露各种应用的指标。scrape manager抓取指标后，将数据交给fanout storage，fanout storage负责数据存储，支持本地存储和外部存储。rule manager根据规则，聚合指标存储到fanout storage，或者生成告警发送给alert manager。Prometheus也实现了各种web接口，使用promQL进行数据的查询。
server的组件包括：
Termination handler. Scrape discovery manager. Notify discovery manager. Scrape manager. Reload handler. Rule manager. TSDB. Web handler. Notifier. 了解了server的基本架构，下面开始研究server的配置。
Prometheus.yml global: scrape_interval: 15s evaluation_interval: 15s rule_files: # - "first.rules" # - "second.rules" scrape_configs: - job_name: prometheus static_configs: - targets: ['localhost:9090'] 这是官网提供的一个简单的配置文件，包含了三个部分：全局配置、规则配置和抓取配置。global包括全局的配置，rule_files配置规则文件，这里没有配置，scrape_configs配置抓取目标。抓取目标是一个列表，包括多个job，每个job有一个job_name，目标配置以及其他配置，目标配置因为支持多种目标源，所以这块在代码中的实现比较复杂。
...</p></div><footer class=entry-footer><span title='2021-03-14 21:32:08 +0800 +0800'>2021-03-14</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to Prometheus内部实现(一)" href=https://solaim.github.io/posts/prometheus%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E4%B8%80/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>K8s Yaml配置文件</h2></header><div class=entry-content><p>1、Redis Deployment部署示例yaml 先来看一个部署yaml示例文件：
# application/guestbook/redis-master-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-master labels: app: redis spec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: labels: app: redis role: master tier: backend spec: containers: - name: master image: redis # or just image: redis resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 通过执行命令kubectl create -f redis-master-deployment.yaml，可以创建1个Redis master Pod。如果我想通过yaml文件创建3个Pod，怎么办？
...</p></div><footer class=entry-footer><span title='2019-11-18 21:23:35 +0800 +0800'>2019-11-18</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to K8s Yaml配置文件" href=https://solaim.github.io/posts/k8s-yaml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>K8s官网留言板demo</h2></header><div class=entry-content><p>https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#start-up-the-redis-master
1、部署redis-master Deployment Step1 编辑redis-master-deployment.yaml # application/guestbook/redis-master-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-master labels: app: redis spec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: labels: app: redis role: master tier: backend spec: containers: - name: master image: redis # or just image: redis resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Step2 执行命令 # 创建Deployment kubectl create -f redis-master-deployment.yaml # 查看pods kubectl get pods 2、部署redis-master Service Step1 编辑redis-master-service.yaml # application/guestbook/redis-master-service.yaml apiVersion: v1 kind: Service metadata: name: redis-master labels: app: redis role: master tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: master tier: backend Step2 执行命令 # 创建DService kubectl create -f redis-master-service.yaml # 查看service kubectl get services 3、部署redis-slave Deployment Step1 编辑redis-slave-deployment.yaml # application/guestbook/redis-slave-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-slave labels: app: redis spec: selector: matchLabels: app: redis role: slave tier: backend replicas: 2 template: metadata: labels: app: redis role: slave tier: backend spec: containers: - name: slave image: gcr.io/google_samples/gb-redisslave:v3 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service's host. To do so, comment out the 'value: dns' line above, and # uncomment the line below: # value: env ports: - containerPort: 6379 Step2 执行命令 kubectl create -f redis-slave-deployment.yaml kubectl get pods 4、部署redis slave Service Step1 编辑redis-slave-service.yaml # application/guestbook/redis-slave-service.yaml apiVersion: v1 kind: Service metadata: name: redis-slave labels: app: redis role: slave tier: backend spec: ports: - port: 6379 selector: app: redis role: slave tier: backend Step2 执行命令 kubectl create -f redis-slave-service.yaml kubectl get services 5、部署guestbook Deployment Step1 编辑frontend-deployment.yaml # application/guestbook/frontend-deployment.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: frontend labels: app: guestbook spec: selector: matchLabels: app: guestbook tier: frontend replicas: 3 template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google-samples/gb-frontend:v6 # 官网上使用的是v4版，在使用中出现ImageInspectErr，镜像出现问题，升级为v6 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service's host. To do so, comment out the 'value: dns' line above, and # uncomment the line below: # value: env ports: - containerPort: 80 Step2 执行命令 kubectl create -f frontend-deployment.yaml kubectl get pods 6、部署guestbook Service Step1 编辑frontend-service.yaml # application/guestbook/frontend-service.yaml apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # comment or delete the following line if you want to use a LoadBalancer type: NodePort # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer ports: - port: 80 selector: app: guestbook tier: frontend Step2 执行命令 kubectl create -f frontend-service.yaml kubectl get services 然后使用node的IP和端口，就可以访问服务了。
...</p></div><footer class=entry-footer><span title='2019-10-20 18:28:40 +0800 +0800'>2019-10-20</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to K8s官网留言板demo" href=https://solaim.github.io/posts/k8s%E5%AE%98%E7%BD%91%E7%95%99%E8%A8%80%E6%9D%BFdemo/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>K8s集群搭建</h2></header><div class=entry-content><p>1、节点 节点 IP master 192.168.124.100 node1 192.168.124.101 node2 192.168.124.102 node3 192.168.124.103 2、docker安装 安装使用官方源，安装命令如下：
# 下载仓库 cd /etc/yum.repos.d wget https://download.docker.com/linux/centos/docker-ce.repo yum makecache fast # 安装docker yum install docker-ce docker-ce-cli containerd.io # 启动docker systemctl enable docker && systemctl start docker 3、kubernetes安装 # 所有节点执行 # 添加仓库 cat &lt;&lt;EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 更新yum cache yum makecache fast # kubeadm初始化会报错, 需要执行下列命令 # 1、关闭firewalld systemctl diable firewalld && systemctl stop firewalld # 2、关闭selinux setenforce 0 && sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config # 3、关闭swap swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # 4、设置网络 touch /etc/sysctl.d/k8s.conf cat > /etc/sysctl.d/k8s.conf &lt;&lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF modprobe br_netfilter && sysctl -p /etc/sysctl.d/k8s.conf # master节点执行 yum install kubeadm kubectl kubelet # 所有node节点执行 yum install kubeadm kubelet # 启动实际运行容器的服务kubelet systemctl enable kubelet && systemctl start kubelet # 现在执行kubelet会报错，不用管 # kubeadm init或join之后，systemd会自动拉起kubelet # kubeadm是集群管理工具 # kubelet负责运行pods # kubectl是客户端，负责和kube-apiserver通信 # 用类比解释的话，kubectl是curl，kube-apiserver是服务器 # kube-controller-manager是控制器 # kube-scheduler是调度器 # kube-proxy负责网络通信 # pause是pod空闲运行的镜像 # etcd用于分布式存储集群信息 # 由于kubeadm使用镜像拉起k8s，但是被墙了，所以在可以翻墙的主机上下载镜像，然后使用下列命令导出镜像 # docker save -o package.tar.gz image1:latest # 使用下面命令导入镜像 # docker load -i package.tar.gz # 获取需要下载的镜像列表 kubeadm config images list # 输出如下，master表示在主节点安装，node表示在从节点安装 k8s.gcr.io/kube-apiserver:v1.16.1 # master k8s.gcr.io/kube-controller-manager:v1.16.1 # master k8s.gcr.io/kube-scheduler:v1.16.1 # master k8s.gcr.io/kube-proxy:v1.16.1 # master node k8s.gcr.io/pause:3.1 # node k8s.gcr.io/etcd:3.3.15-0 # master k8s.gcr.io/coredns:1.6.2 # master node # 很不幸，被墙了 # 用可以翻墙的主机用docker pull拉取镜像，上传本地即可 # 镜像的打包使用save命令 # 镜像的导入使用load命令 # 下载好镜像，导入完毕 # 在master节点执行 kubeadm init --kubernetes-version=v1.16.1 --apiserver-advertise-address=192.168.124.100 --pod-network-cidr=10.244.0.0/16 # 执行成功后，会提示进行如下操作 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 节点加入集群操作 kubeadm join 192.168.124.100:6443 --token ba04v7.y9ki5jgbss8gs0jx \ --discovery-token-ca-cert-hash sha256:8404de0f262daf02de05aad96415c79c2ff39b91ce3ee28cd8248426166123a9 # k8s的网络模型是扁平化的，即pod之间需要能够直接访问，在谷歌的实现中，已经支持扁平化网络模型 # 但是我们创建的集群，不满足这种网络模型，所以需要借助插件或者进行主机配置之后才能满足 # 刚开始学习建议使用flannel，减少障碍，后期有需要进行调研学习 # 安装flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 在master执行 kubectl get nodes # 输出，暂时配置了两个节点 NAME STATUS ROLES AGE VERSION master Ready master 50m v1.16.1 node1 Ready &lt;none> 27m v1.16.1 # 没有安装flannel之前，查看nodes，会提示STATUS为NotReady。 # 至此，k8s的基本安装就结束了。 # 是不是很简单 # 后续会继续分享k8s的学习</p></div><footer class=entry-footer><span title='2019-10-15 21:26:53 +0800 +0800'>2019-10-15</span>&nbsp;·&nbsp;Jerry Wang</footer><a class=entry-link aria-label="post link to K8s集群搭建" href=https://solaim.github.io/posts/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://solaim.github.io/posts/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://solaim.github.io/>Jerry Wang</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>